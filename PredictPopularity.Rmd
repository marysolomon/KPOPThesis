---
title: "Predict Popularity"
author: "Mary Solomon"
date: "2/11/2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(QuantPsyc)#for multivariate normality function
library(data.table)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales)
library(caret)
library(leaps) # for all possible regressions approach LINEAR
library(glmulti)# for all possible regressions approach LOGISTIC
library(moments) # for calculating skewness
```


global functions (convert key to dummy)
```{r}
source("thesis_functions.R")
```


data :D
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               release_date = as.POSIXct(str_sub(data$release_date, 1, 10)),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)),
               popular = factor(ifelse(popularity >=50, 1, 0)))
```

# Understanding Popularity

General Assumptions:
continuous response: popularity score ranging from 0 - 100.
mix of categorical and continuous response.
the distribution of the variables are not normal, we will check for normality of error terms where appropriate.

Goal: create model for predicting popularity scores

## Classification of popularity: Popular vs. Not popular
```{r}
table(data$popular)/nrow(data)
```

if classifying a song as popular when it's score is greater than 50, only ~12% of the data is considered a popular song.




## Assessing normality of popularity scores
```{r}
ggplot(data, aes(x=popularity)) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(data$popularity), sd = sd(data$popularity)), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Popularity") + ylab("Density") +
  ggtitle(paste("Popularity", "Distribution")) +
  theme_bw()
summary(data$popularity)
#ggsave("~/Desktop/KPOPThesis/Figures/Predicting Popularity/popularity_DensityPlot.png")
#dev.off()
```

```{r}
#produce normal qqplot to assess normality
qqnorm(data$popularity, pch = 1, frame = FALSE)
qqline(data$popularity, col = "red", lwd = 2)
#Skewness score
skewness(data$popularity)
kurtosis(data$popularity)
```

moderately skewed right.

### Square Root Transformation
Try square root transformation makes it more normal. This will help to meet the multiple linear regression assumptions.
```{r}
ggplot(data, aes(x=popularity^0.5)) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(data$popularity^0.5), sd = sd(data$popularity^0.5)), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("sqrt(Popularity)") + ylab("Density") +
  ggtitle(paste("sqrt(Popularity)", "Distribution")) +
  theme_bw()
summary(data$popularity^0.5)
# ggsave("~/Desktop/KPOPThesis/Figures/Predicting Popularity/sqrt_popularity_DensityPlot.png")
# dev.off()
```


```{r}
#produce normal qqplot to assess normality
qqnorm(data$popularity^0.5, pch = 1, frame = FALSE)
qqline(data$popularity^0.5, col = "red", lwd = 2)
#Skewness score
skewness(data$popularity^0.5)
kurtosis(data$popularity^0.5)
```




# Multiple Linear Model
select just audio features
```{r}
kpop <- dplyr::select(data, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% filter(mode == 0)%>% dplyr::select(-mode)
kpop1 <- kpop %>% filter(mode == 1) %>% dplyr::select(-mode)

### Kpop mode 0 train and test
smpl.size0 <- floor(0.75*nrow(kpop0))
set.seed(123)
smpl0 <- sample(nrow(kpop0), smpl.size0, replace = FALSE)
train0 <- kpop0[smpl0,]
test0 <- kpop0[-smpl0,]

### Kpop mode 1 train and test
smpl.size1 <- floor(0.75*nrow(kpop1))
set.seed(123)
smpl1 <- sample(nrow(kpop1), smpl.size1, replace = FALSE)
train1 <- kpop1[smpl1,]
test1 <- kpop1[-smpl1,]
```



## For Songs with Mode 0

### Multiple Linear Regression Assumption Checking
fit a multiple linear regression model
```{r}
ml0 <- lm(popularity ~. , data = train0)
summary(ml0)
```



```{r, width = 10, height = 10}
par(mfrow = c(2,2))
plot(ml0)
```

Interpretting the diagnostic plots: 

* Residuals vs. Fitted: The line through this plot is somewhat horizontal but has a polynomial curve like pattern throught the 0 horizontal line. This indicates that there could be a linear relationship.

* Normal Q-Q: The residuals roughly follow the diagonal Normal QQ line, indicating that the residuals are normally distributed.

* Scale Location: The red line does not pass through the scale-location graph horizontally, rather in a slight positive direction. Therefore, our residuals indicate a heteroscedasticity problem. In otherwords, we do not satisfy the MLR requirement of equal variance across residuals.

* Residuals vs. Leverage: There are residuals that reach above standard residual 3, therefore, there are many outliers.

Try again with tranformation to make popularity normal:(to the squareroot)
```{r}
ml0.sqrt <- lm(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5))
summary(ml0.sqrt)
```

assumption checking and diagnostics
```{r}
png(filename = "~/Desktop/KPOPThesis/Figures/Predicting Popularity/sqrt_popularity_diagnostics_mode0.png")
par(mfrow = c(2,2))
plot(ml0.sqrt)
dev.off
```

Interpretting the diagnostic plots: 

* Residuals vs. Fitted: The line through this plot is somewhat horizontal but still exhibits some curvature. Overall though, we could conclude that there is roughly a linear relationship between the predictors and outcome variables.

* Normal Q-Q: The residuals roughly follow the diagonal Normal QQ line, indicating that the residuals are normally distributed. In comparison to the original scale of the data, there is slightly more deviance on the upper tail with the points falling below the normal QQ line.

* Scale Location: The red line is roughly more horizontal. Therefore, our residuals exhibit heteroscedasticity. In other words, the model now satisfies the MLR requirement of equal variance across residuals.

* Residuals vs. Leverage: Most of the residuals fall with in the standardized residual valuse of -3 and 3, therefore, we have few outliers in the dataset. 

### Full MLR Model

```{r}
summary(ml0.sqrt)
```

The full Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 12.803 - 0.010{Duration} - 0.142{Acousticness} - 0.138{Danceability} - 3.909{Energy} - 2.244{Instrumentalness} 
\\
- 0.131{Key1} - 0.375{Key2} + 0.182{Key3} - 0.544{Key4} - 0.119{Key5} + 0.138{Key6} - 0.024{Key7} 
\\ 
- 0.157{Key8} - 0.152{Key9} - 0.235{Key10} - 0.291{Key11} + 0.423{Loudness} + 2.157{Speechiness}
\\
-0.001{Tempo} - 1.189{Valence}
$$

The variables that have significant contribution in predicting the popularity score is Duration, Energy, Instrumentalness, Key4 (E Minor), Loudness, Speechiness, and Valence. 

An increase in Duration, Acousticness, Danceability, Energy, Instrumentalness, Tempo, Valence and choosing Key1 (C#/D-flat Minor), Key2 (D Minor), Key4 (E Minor), Key5 (F Minor), Key7 (G Minor), Key8 (G#/A-flat Minor), Key9 (A Minor), Key10 (A#/B-flat Minor), Key11 (B minor) instead of Key0(Cminor) decreases the square root popularity score, on average. An increase in Loudness, Speechiness and choosing Key3 (D#/E-flat Minor) or Key6 (F#/G-flat Minor)over Key0 (C Minor) increases the square root popularity score, on average.

prediction on test data
```{r}
# prediction on test data
yhat.mlr = predict(ml0.sqrt, newdata = test0 %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.mlr <- yhat.mlr - test0$popularity^0.5
rmse.mlr <- sqrt(mean(error.mlr^2))
data.frame(
  RMSE = RMSE(yhat.mlr, test0$popularity^0.5),
  R2 = R2(yhat.mlr, test0$popularity^0.5)
)
```




### Variable Selection: Stepwise 10 fold cross validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5),  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv)
summary(mlr.step_kcv$finalModel)
```


The Stepwise 10 fold Cross Validation Multiple Linear Regression model can be written:
$$
\hat {\sqrt {popularity}} = 12.404 - 0.010{Duration} - 3.801{Energy} - 2.244{Instrumentalness} - 0.259{Key2} + 0.305{Key3} - 0.424{Key4}
\\
 + 0.262{Key6}  - 0.169{Key11} + 0.421{Loudness} + 2.100{Speechiness} - 1.189{Valence}
$$

Compared to the full Multiple Linear Regression Model, the Stepwise method removes the variables: Acousticness, Danceability, Key1, Key5, Key7 - Key10, and Tempo.

The variables that have significant contribution in predicting the popularity score is Duration, Energy, Instrumentalness, Key4 (E Minor), Key6 (F#/G-flat Minor), Loudness, Speechiness, and Valence. 

An increase in Duration, Energy, Instrumentalness, Valence and choosing, Key2 (D Minor), Key4 (E Minor), Key5 (F Minor), , Key11 (B minor) instead of Key0(Cminor) decreases the square root popularity score, on average. An increase in Loudness, Speechiness and choosing Key 3 (D#/E-flat Minor) or Key6 (F#/G-flat Minor) over Key0 (C Minor) increases the square root popularity score, on average.


prediction on test data
```{r}
# prediction on test data
yhat.step_kcv = predict(mlr.step_kcv$finalModel, newdata=key.dummy(test0) %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.step_kcv <- yhat.step_kcv - test0$popularity^0.5
rmse.step_kcv <- sqrt(mean(error.step_kcv^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv, test0$popularity^0.5),
  R2 = R2(yhat.step_kcv, test0$popularity^0.5)
)
```

Both of the models have RMSE scores of around 2. This isn't terrible since the range of the transformed popularity scores is 0-10.


## Variable Selection: All Subsets Regression (Not using K-folds CV)

```{r}
set.seed(123)
allreg.models0 <- regsubsets(popularity ~., data = train0 %>% mutate(popularity = popularity^0.5), nvmax = 21)
summary(allreg.models0)
```

Assess models
```{r}
allreg.res0 <- summary(allreg.models0)
allreg.compare0 <- data.frame(model = c(1:20),
                              Adj.R2 = allreg.res0$adjr2,
                              CP = allreg.res0$cp)
allreg.compare0
```

Optimal combination of low CP and Adj.R2, is associated with model number 11.

```{r}
set.seed(123)
mlr.allreg0 <- lm(popularity ~ duration + energy + instrumentalness +
  key2+key3+key4 +key6 +key11 +loudness +speechiness+valence, data = key.dummy(train0 %>% mutate(popularity = popularity^0.5)))
summary(mlr.allreg0)
```

The All subsets regression returns a nearly identical model as the stepwise regression method, the only difference is the coefficient for valence
$$
\hat {\sqrt {popularity}} = 12.404 - 0.010{Duration} - 3.801{Energy} - 2.244{Instrumentalness} - 0.259{Key2} + 0.305{Key3} - 0.424{Key4}
\\
 + 0.262{Key6}  - 0.169{Key11} + 0.421{Loudness} + 2.100{Speechiness} - 1.233{Valence}
$$

prediction on test data
```{r}
# prediction on test data
yhat.allreg0 = predict(mlr.allreg0, newdata=key.dummy(test0 %>% mutate(popularity = popularity^0.5)))
data.frame(
  RMSE = RMSE(yhat.allreg0, test0$popularity^0.5),
  R2 = R2(yhat.allreg0, test0$popularity^0.5)
)
```

### Regularized Regression: Ridge
```{r}
lm.ridge0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge0$bestTune
# best coefficient
lm.ridge0.model <- coef(lm.ridge0$finalModel, lm.ridge0$bestTune$lambda)
lm.ridge0.model
```

The Ridge Multiple Linear Regularized Regression model can be written 
$$
\hat {\sqrt {popularity}} = 12.341 - 0.010{Duration} - 0.066{Acousticness} - 0.149{Danceability} - 3.560{Energy} - 2.269{Instrumentalness} 
\\
- 0.083{Key1} - 0.324{Key2} + 0.226{Key3} - 0.493{Key4} - 0.070{Key5} + 0.184{Key6} + 0.017{Key7} 
\\ 
- 0.106{Key8} - 0.108{Key9} - 0.184{Key10} - 0.245{Key11} + 0.399{Loudness} + 2.040{Speechiness}
\\
-0.001{Tempo} - 1.175{Valence}
$$


An increase in Duration, Acousticness, Danceability, Energy, Instrumentalness, Tempo, Valence and choosing Key1 (C#/D-flat Minor), Key2 (D Minor), Key4 (E Minor), Key5 (F Minor), Key8 (G#/A-flat Minor), Key9 (A Minor), Key10 (A#/B-flat Minor), Key11 (B minor) instead of Key0(Cminor) decreases the square root of the popularity score, on average. An increase in Loudness, Speechiness and choosing Key3 (D#/E-flat Minor), Key6 (F#/G-flat Minor), or Key7 (G Minor)over Key0 (C Minor) increases the square root of the popularity score, on average.

In comparison to the Full MLR model, the ridge shrinks the coefficients closer to zero, and the coefficient of dummy variable Key7 flipped signs from having a negative coefficient to a positive coefficient.


```{r}
# prediction on test data
yhat.ridge0 = predict(lm.ridge0, s=lm.ridge0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge0, test0$popularity^0.5),
  R2 = R2(yhat.ridge0, test0$popularity^0.5)
)
```

### Regularized Regression: Lasso
```{r}
lm.lasso0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso0$bestTune
# best coefficient
lm.lasso0.model <- coef(lm.lasso0$finalModel, lm.lasso0$bestTune$lambda)
lm.lasso0.model
```

The Lasso Multiple Linear Regularized Regression model can be written 
$$
\hat {\sqrt {popularity}} = 11.897 - 0.010{Duration} - 3.492{Energy} - 2.047{Instrumentalness} 
\\
- 0.155{Key2} + 0.212{Key3} - 0.359{Key4} + 0.221{Key6} - 0.033{Key7} - 0.040{Key10} 
\\ 
- 0.118{Key11} + 0.398{Loudness} + 1.755{Speechiness} - 1.135{Valence}
$$

Compared to the full Multiple Linear Regression model, the Lasso model removes variables: Acousticness, Danceability, Key1, Key5, Key8, Key9, and Tempo

An increase in Duration, Energy, Instrumentalness, Valence and choosing Key2 (D Minor), Key4 (E Minor), Key7 (G Minor), Key10 (A#/B-flat Minor), Key11 (B minor) instead of Key0(Cminor) decreases the square root popularity score, on average. An increase in Loudness, Speechiness and choosing Key3 (D#/E-flat Minor) or Key6 (F#/G-flat Minor)over Key0 (C Minor) increases the square root popularity score, on average.


```{r}
# prediction on test data
yhat.lasso0 = predict(lm.lasso0, s=lm.lasso0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.lasso0 <- yhat.lasso0 - test0$popularity^0.5
rmse.lasso0 <- sqrt(mean(error.lasso0^2))
data.frame(
  RMSE = RMSE(yhat.lasso0, test0$popularity^0.5),
  R2 = R2(yhat.lasso0, test0$popularity^0.5)
)
```

### Regularized Regression: Elastic Net
```{r}
lm.enet0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lm.enet0$bestTune
# best coefficient
lm.enet0.model <- coef(lm.enet0$finalModel, lm.enet0$bestTune$lambda)
lm.enet0.model
```

The Elastic Net Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 11.982 - 0.010{Duration} - 3.542{Energy} - 2.081{Instrumentalness}  - 0.174{Key2}
\\
+ 0.227{Key3} - 0.371{Key4} + 0.227{Key6}+  0.043{Key7} - 0.053{Key10}  - 0.127{Key11}
\\
 + 0.402{Loudness} + 1.817{Speechiness}-0.000001{Tempo} - 1.150{Valence}
$$

Compare to the Full Multiple linear regression model, the Elastic Net regression removes the variables: Acousticness, Danceability, Key1, Key5, Key8, and Key9. The Key7 Dummy variable also has a positive coefficient rather than the negative coefficient in the full model.

An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing Key2 (D Minor), Key4 (E Minor), Key10 (A#/B-flat Minor), Key11 (B minor) instead of Key0(Cminor) decreases the square root popularity score, on average. An increase in Loudness, Speechiness and choosing Key3 (D#/E-flat Minor), Key6 (F#/G-flat Minor) or Key7 (G Minor) instead of Key0 (C Minor) increases the square root popularity score, on average.



```{r}
# prediction on test data
yhat.enet0 = predict(lm.enet0, s=lm.enet0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.enet0 <- yhat.enet0 - test0$popularity^0.5
rmse.enet0 <- sqrt(mean(error.enet0^2))
data.frame(
  RMSE = RMSE(yhat.enet0, test0$popularity^0.5),
  R2 = R2(yhat.enet0, test0$popularity^0.5)
)
```


### Deciding the best MLR model for mode 0 songs
```{r}
mlr0.results <- data.frame(Model = c("Full", "Step 10CV", "All Subsets", "Ridge 10CV", "Lasso 10CV", "ElasticNet 10CV"),
                           #AIC = c(),
                           adjR2 = c(0.1546, 0.1560, 0.1560, NA, NA,NA),
                           RMSE = c(1.8635, 1.8612, 1.8612, 1.8611, 1.8588, 1.8592),
                           Pred.R2 = c( 0.1333, 0.1351,  0.1351, 0.1340, 0.1352, 0.1351))
mlr0.results
```

The model that yields the lowest RMSE and highest R2 from predictions is the Lasso Multiple Linear Regularized Regression model.

The Lasso Multiple Linear Regularized Regression model can be written 
$$
\hat {\sqrt {popularity}} = 11.897 - 0.010{Duration} - 3.492{Energy} - 2.047{Instrumentalness} 
\\
- 0.155{Key2} + 0.212{Key3} - 0.359{Key4} + 0.221{Key6} - 0.033{Key7} - 0.040{Key10} 
\\ 
- 0.118{Key11} + 0.398{Loudness} + 1.755{Speechiness} - 1.135{Valence}
$$


## For songs with Mode 1

### Assumption checking

```{r}
ml1 <- lm(popularity ~. , data = train1)
plot(ml1)
```

```{r}
png(filename = "~/Desktop/KPOPThesis/Figures/Predicting Popularity/sqrt_popularity_diagnostics_mode1.png")
par(mfrow = c(2,2))
ml1.sqrt <- lm(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5))
plot(ml1.sqrt)
dev.off()
```



### Full Multiple linear model



```{r}
summary(ml1.sqrt)
```


The full Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 10.857 - 0.010{Duration} + 0.081{Acousticness} + 0.051{Danceability} - 3.078{Energy} - 1.877{Instrumentalness} 
\\
+ 0.321{Key1} + 0.207{Key2} + 0.431{Key3} - 0.029{Key4} + 0.204{Key5} + 0.217{Key6} + 0.283{Key7} 
\\ 
+ 0.297{Key8} + 0.271{Key9} + 0.080{Key10} + 0.390{Key11} + 0.386{Loudness} + 4.580{Speechiness}
\\
-0.001{Tempo} - 0.849{Valence}
$$

The variables that have significant contribution in predicting the popularity score is Duration, Energy, Instrumentalness, Key1 (C#/D-flat Major), Key2, Key3, Key6, Key7, Key8, Key9, Key11, Loudness, Speechiness, and Valence. 

An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing  Key4 (E Major) instead of Key0(C Major) decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), Key10 (A#/B-flat Major),or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.


prediction on test data
```{r}
# prediction on test data
yhat.mlr = predict(ml1.sqrt, newdata = test1 %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.mlr <- yhat.mlr - test1$popularity^0.5
rmse.mlr <- sqrt(mean(error.mlr^2))
data.frame(
  RMSE = RMSE(yhat.mlr, test1$popularity^0.5),
  R2 = R2(yhat.mlr, test1$popularity^0.5)
)
```


### Variable Selection: Stepwise 10 fold cross validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5),  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv)
summary(mlr.step_kcv$finalModel)
```

The Stepwise Multiple Linear Regression model can be written: 
$$
\hat {\sqrt {popularity}} = 10.985 - 0.010{Duration} - 3.161{Energy} - 1.884{Instrumentalness} 
\\
+ 0.312{Key1} + 0.199{Key2} + 0.425{Key3} + 0.196{Key5} + 0.210{Key6} + 0.275{Key7} 
\\ 
+ 0.289{Key8} + 0.262{Key9} + 0.383{Key11} + 0.387{Loudness} + 4.579{Speechiness}
\\
-0.002{Tempo} - 0.837{Valence}
$$

Compared to the full multiple linear regression model, the stepwise model removes the variables Acousticness, Danceability, Key4, Key10

The variables that have significant contribution in predicting the popularity score is Duration, Energy, Instrumentalness, Key1 (C#/D-flat Major), Key2, Key3, Key6, Key7, Key8, Key9, Key11, Loudness, Speechiness, and Valence. 

An increase in Duration, Energy, Instrumentalness, Tempo, and Valence decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.



prediction on test data
```{r}
# prediction on test data
yhat.step_kcv = predict(mlr.step_kcv$finalModel, newdata=key.dummy(test1) %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.step_kcv <- yhat.step_kcv - test1$popularity^0.5
rmse.step_kcv <- sqrt(mean(error.step_kcv^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv, test1$popularity^0.5),
  R2 = R2(yhat.step_kcv, test1$popularity^0.5)
)
```


### Variable Selection: All Subsets Regression (Not using K-folds CV)

```{r}
set.seed(123)
allreg.models1 <- regsubsets(popularity ~., data = train1 %>% mutate(popularity = popularity^0.5), nvmax = 21)
summary(allreg.models1)
```

Assess models
```{r}
allreg.res1 <- summary(allreg.models1)
allreg.compare1 <- data.frame(model = c(1:20),
                              Adj.R2 = allreg.res1$adjr2,
                              CP = allreg.res1$cp)
allreg.compare1
```

The model with the best balance for high adjusted r2 and low CP value is model 16
```{r}
set.seed(123)
mlr.allreg1 <- lm(popularity ~ duration + energy + instrumentalness +
 key1 + key2+key3+key5 +key6 + key7 + key8 + key9 +key11 +loudness +speechiness+ tempo + valence, data = key.dummy(train1 %>% mutate(popularity = popularity^0.5)))
summary(mlr.allreg1)
```

$$
\hat {\sqrt {popularity}} = 10.985 - 0.009{Duration} - 3.161{Energy} - 1.884{Instrumentalness} 
\\
+ 0.312{Key1} + 0.199{Key2} + 0.425{Key3} + 0.196{Key5} + 0.210{Key6} + 0.275{Key7} 
\\ 
+ 0.289{Key8} + 0.262{Key9} + 0.383{Key11} + 0.387{Loudness} + 4.579{Speechiness}
\\
-0.002{Tempo} - 0.837{Valence}
$$

prediction on test data
```{r}
# prediction on test data
yhat.allreg1 = predict(mlr.allreg1, newdata=key.dummy(test1 %>% mutate(popularity = popularity^0.5)))
data.frame(
  RMSE = RMSE(yhat.allreg1, test1$popularity^0.5),
  R2 = R2(yhat.allreg1, test1$popularity^0.5)
)
```


### Regularized Regression: Ridge
```{r}
lm.ridge1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge1$bestTune
# best coefficient
lm.ridge1.model <- coef(lm.ridge1$finalModel, lm.ridge1$bestTune$lambda)
lm.ridge1.model
```

The Ridge Multiple Linear Regularized Regression model can be written:  
$$
\hat {\sqrt {popularity}} = 10.348 - 0.010{Duration} + 0.170{Acousticness} + 0.050{Danceability} - 2.621{Energy} - 1.928{Instrumentalness} 
\\
+ 0.294{Key1} + 0.183{Key2} + 0.399{Key3} - 0.049{Key4} + 0.179{Key5} + 0.189{Key6} + 0.260{Key7} 
\\ 
+ 0.273{Key8} + 0.249{Key9} + 0.052{Key10} + 0.361{Key11} + 0.356{Loudness} + 4.325{Speechiness}
\\
-0.001{Tempo} - 0.859{Valence}
$$

An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing  Key4 (E Major) instead of Key0(C Major) decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), Key10 (A#/B-flat Major),or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.




```{r}
# prediction on test data
yhat.ridge1 = predict(lm.ridge1, s=lm.ridge1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge1, test1$popularity^0.5),
  R2 = R2(yhat.ridge1, test1$popularity^0.5)
)
```


### Regularized Regression: Lasso
```{r}
lm.lasso1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso1$bestTune
# best coefficient
lm.lasso1.model <- coef(lm.lasso1$finalModel, lm.lasso1$bestTune$lambda)
lm.lasso1.model
```

The Lasso Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 10.823 - 0.010{Duration} + 0.079{Acousticness} + 0.007{Danceability} - 3.012{Energy} - 1.855{Instrumentalness} 
\\
+ 0.269{Key1} + 0.153{Key2} + 0.368{Key3} - 0.061{Key4} + 0.148{Key5} + 0.160{Key6} + 0.232{Key7} 
\\ 
+ 0.243{Key8} + 0.216{Key9} + 0.020{Key10} + 0.334{Key11} + 0.381{Loudness} + 4.504{Speechiness}
\\
-0.001{Tempo} - 0.830{Valence}
$$

Compared to the full multiple linear regression model, all variables are kept in the model with lasso, but the coefficients are shrunk closer to zero.

An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing  Key4 (E Major) instead of Key0 (C Major) decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), Key10 (A#/B-flat Major),or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.

```{r}
# prediction on test data
yhat.lasso1 = predict(lm.lasso1, s=lm.lasso1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.lasso1 <- yhat.lasso1 - test1$popularity^0.5
rmse.lasso1 <- sqrt(mean(error.lasso1^2))
data.frame(
  RMSE = RMSE(yhat.lasso1, test1$popularity^0.5),
  R2 = R2(yhat.lasso1, test1$popularity^0.5)
)
```

### Regularized Regression: Elastic Net
```{r}
lm.enet1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lm.enet1$bestTune
# best coefficient
lm.enet1.model <- coef(lm.enet1$finalModel, lm.enet1$bestTune$lambda)
lm.enet1.model
```


The Elastic Net Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 10.771 - 0.010{Duration} + 0.092{Acousticness} + 0.027{Danceability} - 2.979{Energy} - 1.872{Instrumentalness} 
\\
+ 0.288{Key1} + 0.173{Key2} + 0.391{Key3} - 0.050{Key4} + 0.169{Key5} + 0.181{Key6} + 0.251{Key7} 
\\ 
+ 0.263{Key8} +  0.237{Key9} + 0.042{Key10} + 0.354{Key11} + 0.379{Loudness} + 4.504{Speechiness}
\\
-0.001{Tempo} - 0.841{Valence}
$$


An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing  Key4 (E Major) instead of Key0(C Major) decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), Key10 (A#/B-flat Major),or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.


```{r}
# prediction on test data
yhat.enet1 = predict(lm.enet1, s=lm.enet1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.enet1 <- yhat.enet1 - test1$popularity^0.5
rmse.enet1 <- sqrt(mean(error.enet1^2))
data.frame(
  RMSE = RMSE(yhat.enet1, test1$popularity^0.5),
  R2 = R2(yhat.enet1, test1$popularity^0.5)
)
```

### Deciding the best MLR model for mode 1 songs
```{r}
mlr1.results <- data.frame(Model = c("Full", "Step 10CV", "All Subsets","Ridge 10CV", "Lasso 10CV", "ElasticNet 10CV"),
                           #AIC = c(),
                           adjR2 = c(0.1468, 0.1453, 0.1473,NA, NA, NA),
                           RMSE = c(1.858473, 1.858518, 1.858518, 1.858182, 1.858393, 1.858386),
                           Pred.R2 = c(0.1257546, 0.1257011, 0.1257011, 0.1253763, 0.1255584, 0.1255727))
mlr1.results
```

The model with the lowest RMSE and highest R2 is the Full multiple linear regression model.

The full Multiple Linear Regression model can be written 
$$
\hat {\sqrt {popularity}} = 10.857 - 0.010{Duration} + 0.081{Acousticness} + 0.051{Danceability} - 3.078{Energy} - 1.877{Instrumentalness} 
\\
+ 0.321{Key1} + 0.207{Key2} + 0.431{Key3} - 0.029{Key4} + 0.204{Key5} + 0.217{Key6} + 0.283{Key7} 
\\ 
+ 0.297{Key8} + 0.271{Key9} + 0.080{Key10} + 0.390{Key11} + 0.386{Loudness} + 4.580{Speechiness}
\\
-0.001{Tempo} - 0.849{Valence}
$$

The variables that have significant contribution in predicting the popularity score is Duration, Energy, Instrumentalness, Key1 (C#/D-flat Major), Key2, Key3, Key6, Key7, Key8, Key9, Key11, Loudness, Speechiness, and Valence. 

An increase in Duration, Energy, Instrumentalness, Tempo, Valence and choosing  Key4 (E Major) instead of Key0(C Major) decreases the square root popularity score, on average. An increase in Acousticness, Danceability, Loudness, Speechiness and choosing Key1 (C#/D-flat Major), Key2 (D Major), Key3 (D#/E-flat Major), Key5 (F Major), Key6 (F#/G-flat Minor), Key7 (G Major), Key8 (G#/A-flat Major), Key9 (A Major), Key10 (A#/B-flat Major),or Key11 (B Major) over Key0 (C Major) increases the square root popularity score, on average.


# Logistic (classification approach)

Use 70% train, 15% validation, 15% test,  to use validation for finding optimal cutoff value.
```{r}
kpop.logit <- dplyr::select(data, popular, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
logit.kpop0 <- kpop.logit %>% filter(mode == 0)%>% dplyr::select(-mode)
logit.kpop1 <- kpop.logit %>% filter(mode == 1) %>% dplyr::select(-mode)

set.seed(123)
p3 <- partition.3(logit.kpop0, 0.70, 0.15)
logit.train0 <- p3$data.train
logit.valid0 <- p3$data.val
logit.test0 <- p3$data.test
all.train0 <- rbind(logit.train0, logit.valid0)

set.seed(123)
p3 <- partition.3(logit.kpop1, 0.70, 0.15)
logit.train1 <- p3$data.train
logit.valid1 <- p3$data.val
logit.test1 <- p3$data.test
all.train1 <- rbind(logit.train1, logit.valid1)
```


## Mode 0 popularity

### Full Logistic Model: train/valid/test

fitting logistic model using combo of train/valid/test, finding optimal model using training data.
```{r, warning = FALSE}
# Fit logistic model on training data
v.logit.model0 <- glm(popular ~ ., family=binomial(link='logit'),data= logit.train0)

#search for best cutoff
out0 <- opt.cut.func(v.logit.model0, logit.valid0)
opt.cut.plot(out0)
out0$cutoff[which.min(out0$ssdiff.vec)]
v.opt.cut0 <- out0$cutoff[which.max(out0$kappa.vec)]
v.opt.cut0
```

Fit final model (combo of train and validation)
```{r}
v.model.final <-  glm(popular ~ ., data=all.train0, family=binomial(link='logit'))
summary(v.model.final)
```

Full Logistic Model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 5.059 - 0.009{Duration} - 1.472{Acousticness} + 0.311{Danceability} - 3.723{Energy} -2.809{Instrumentalness}
\\
+ 0.111{Key1} - 0.088{Key2} + 0.772{Key3} - 0.327{Key4} + 0.180{Key5} + 0.314{Key6} + 0.175{Key7} + 0.218{Key8} + 0.032{Key9}
\\
+ 0.152{Key10} + 0.059{Key11} + 0.350{Loudness} + 3.453{Speechiness} - 0.001{Tempo} -1.639{Valence}
$$

Significant variables are the Duration, Acousticness, Energy, Loudness, Speechiness, and Valence and having Key3 (D#/E-flat Minor) instead of Key0 (C Minor).


predict on test using 0.5 cutoff 
```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > 0.5, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```



predict on optimal cutoff (0.18) for highest kappa
```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > v.opt.cut0, 1, 0) # using cutoff = 0.18
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```

Predict on optimal balance between sensitivity and specificity (0.14)
```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > 0.14, 1, 0) # using cutoff = 0.14
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```


### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
meh <- capture.output(step0_kcv <- train(popular ~ ., data = logit.train0, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step0_kcv <- step0_kcv$finalModel
step0_kcv
summary(step0_kcv)
```


```
kcv.prob.test0 <- predict(step0_kcv, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

Find optimal cut off value:
```{r, warning = FALSE}
#search for best cutoff
kcv.out0 <- opt.cut.func(step0_kcv, key.dummy(logit.valid0))
opt.cut.plot(kcv.out0)
kcv.out0$cutoff[which.min(kcv.out0$ssdiff.vec)]
kcv.out0.cut0 <- kcv.out0$cutoff[which.max(kcv.out0$kappa.vec)]
kcv.out0.cut0
```

Fit final model (combo of train and validation)
```{r}
set.seed(123)
finalmeh <- capture.output(step0_kcv.final0 <- train(popular ~ ., data = all.train0, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
step0_kcv.final0 <- step0_kcv.final0$finalModel
step0_kcv.final0
summary(step0_kcv.final0)
```


Stepwise 10 Fold Cross Validation:

Compared to the full logistic regression model, the stepwise model leaves out Danceability, Key1, Key2, Key5 - Key11, and Tempo.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 5.377 - 0.009{Duration} - 1.525{Acousticness} - 3.860{Energy} - 2.739{Instrumentalness} 
\\
 + 0.639{Key3} - 0.443{Key4} + 0.351{Loudness} + 3.328{Speechiness} - 1.522{Valence}
$$

The Significant variables in the model are Duration, Acousticness, Energy, Key3 (D#/E-flat Minor), Key4 (E Minor) Loudness, Speechiness and Valence.

predict on test using 0.5 cutoff 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

predict on test using optimal kappa, 0.17 cutoff 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.17, 1, 0) # using cutoff = 0.17
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

predict on test using 0.14 cutoff, optimal balance between sensitivity and specificity 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.14, 1, 0) # using cutoff = 0.16
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


### Variable Selection: All possible regression

```{r}
set.seed(123)
glmulti.out0 <- glmulti(popular ~ ., data = logit.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out0@formulas
```

view summary of top model
```{r}
summary(glmulti.out0@objects[[1]])
```


Store model
```{r}
allreg.logit0 <- glmulti.out0@objects[[1]]
```

```{r, warning = FALSE}
#search for best cutoff
allreg.out0 <- opt.cut.func(allreg.logit0, logit.valid0)
opt.cut.plot(allreg.out0)
allreg.out0$cutoff[which.min(allreg.out0$ssdiff.vec)]
allreg.out0.cut0 <- allreg.out0$cutoff[which.max(allreg.out0$kappa.vec)]
allreg.out0.cut0
```

fit final model to combo of training and validation
```{r}
set.seed(123)
glmulti.out0 <- glmulti(popular ~ ., data = all.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
summary(glmulti.out0@objects[[1]])
```

All Possible Regression:

Compared to the full Logistic model, the Model from All subsets regression leaves out the variables Danceability, all Key dummy variables, and Tempo.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 5.369 - 0.009{Duration} - 1.537{Acousticness} - 3.955{Energy} - 2.722{Instrumentalness}
\\
 + 0.354{Loudness} + 3.278{Speechiness} - 1.445{Valence}
$$ 


store final model
```{r}
allreg.logit0.final <- glmulti.out0@objects[[1]]
```

Predictions with 0.5 as the cut off
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > 0.5, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


Predictions where cut off yields the best kappa, 0.14 
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > allreg.out0.cut0, 1, 0) # using cutoff 0.14
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


Predictions where cut off is the best balance of sensitivity and specificity, 0.15
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > 0.15, 1, 0) # using cutoff 0.15
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
set.seed(123)
ridge0 <- train(popular ~ ., data = logit.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0$bestTune
ridge0.model <- coef(ridge0$finalModel, ridge0$bestTune$lambda)
ridge0.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
ridge0.out <- reg.opt.cut.func(ridge0, logit.valid0)
opt.cut.plot(ridge0.out)
# cut off by kappa
ridge0.out$cutoff[which.max(ridge0.out$kappa.vec)]
ridge0.out$cutoff[which.min(ridge0.out$ssdiff.vec)]
```


create final model
```{r}
set.seed(123)
ridge0 <- train(popular ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0$bestTune
ridge0.model <- coef(ridge0$finalModel, ridge0$bestTune$lambda)
ridge0.model
```



Ridge 10 Fold Cross Validation:

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.322 - 0.004{Duration} - 0.409{Acousticness} - 0.063{Danceability} - 0.607{Energy} - 0.839{Instrumentalness}
\\
+ 0.013{Key1} - 0.091{Key2} + 0.401{Key3} - 0.053{Key4} + 0.054{Key5} + 0.142{Key6} + 0.037{Key7} + 0.030{Key8} - 0.042{Key9}
\\
+ 0.034{Key10} - 0.034{Key11} + 0.084{Loudness} + 1.616{Speechiness} - 0.0003{Tempo} - 0.724{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge0 <- predict(ridge0, s = ridge0$bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.15 corresponding to optimal kappa
```{r}
prob.ridg0 <- predict(ridge0, s = ridge0bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.15, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```


predict and evaluate on the test set where cutoff is at 0.14 corresponding to optimal sensitivity and specificity balance
```{r}
prob.ridg0 <- predict(ridge0, s = ridge0bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.14, 1, 0) # using cutoff = 0.14
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```

### Regularized Regression: Lasso 10 fold Cross Validation (returns with a model of just the intercept)
```{r}
lasso0 <- train(popular ~ ., data = logit.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso0$bestTune
# best coefficient
lasso0.model <- coef(lasso0$finalModel, lasso0$bestTune$lambda)
lasso0.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
lasso0.out <- reg.opt.cut.func(lasso0, logit.valid0)
opt.cut.plot(lasso0.out)
# cut off by kappa
lasso0.out$cutoff[which.max(lasso0.out$kappa.vec)]
lasso0.out$cutoff[which.min(lasso0.out$ssdiff.vec)]

```


```{r}
lasso0.final <- train(popular ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso0.final$bestTune
# best coefficient
lasso0.model.final <- coef(lasso0.final$finalModel, lasso0.final$bestTune$lambda)
lasso0.model.final
```

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.lasso0 <- predict(lasso0.final, s = lasso0.final$bestTune, logit.test0, type = "prob")
pred.lasso0 <- ifelse(prob.lasso0[,2] > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.lasso0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11
```{r}
prob.lasso0 <- predict(lasso0.final, s = lasso0.final$bestTune, logit.test0, type = "prob")
pred.lasso0 <- ifelse(prob.lasso0[,2] > 0.11, 1, 0) # using cutoff = 0.11
confusionMatrix(as.factor(pred.lasso0), as.factor(logit.test0$popular), 
                positive = "1")
```



### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
set.seed(123)
enet0 <- train(popular ~ ., data = logit.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0$bestTune
# best coefficient
enet0.model <- coef(enet0$finalModel, enet0$bestTune$lambda)
enet0.model
```

search for best cutoff with validation set
```{r, warning = FALSE}
enet0.out <- reg.opt.cut.func(enet0, logit.valid0)
opt.cut.plot(enet0.out)
# cut off by kappa
enet0.out$cutoff[which.max(enet0.out$kappa.vec)]
enet0.out$cutoff[which.min(enet0.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
set.seed(123)
enet0 <- train(popular ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0$bestTune
# best coefficient
enet0.model <- coef(enet0$finalModel, enet0$bestTune$lambda)
enet0.model
```


Elastic Net 10 Cross Validation
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.322 - 0.004{Duration} - 0.409{Acousticness} - 0.063{Deanceability} - 0.607{Energy} - 0.839{Instrumentalness}
\\
+ 0.013{Key1} - 0.091{Key2} + 0.401{Key3} - 0.198{Key4} + 0.054{Key5} + 0.142{Key6} + 0.037{Key7} + 0.030{Key 8}
\\ 
- 0.041{Key9} + 0.034{Key10} - 0.034{Key11} + 0.084{Loudness} + 1.616{Speechiness} - 0.0003{Tempo} - 0.724{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.15 corresponding to optimal kappa
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.15, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.14 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.14, 1, 0) # using cutoff = 0.14
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```

### Decide on the best logistic model for Mode 0 


Diagnostic measures
```{r}
results.logit0 <- data.frame(model = c("Full", "Step 10CV", "All Subsets", "Ridge 10CV", "Elastic Net 10CV"),
                             cutoff = c(0.14, 0.14, 0.14, 0.14, 0.14),
                             AIC = c(2946, 2928, 2938.2, NA, NA),
                             Accuracy =c(0.6205, 0.6362, 0.6505, 0.6405, 0.6405),
                             Kappa = c(0.1013, 0.1007, 0.1269, 0.0901, 0.0901),
                             Sensitivity = c(0.55319, 0.5213, 0.55319, 0.48936, 0.48936),
                             Specificity = c(0.63097, 0.6540, 0.66557, 0.66392, 0.66392))
results.logit0
```

The best model is the all subsets regression method model as it has the highest accuracy, sensitivity and specificity values. 

All Possible Regression:

Compared to the full Logistic model, the Model from All subsets regression leaves out the variables Danceability, all Key dummy variables, and Tempo.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 5.369 - 0.009{Duration} - 1.537{Acousticness} - 3.955{Energy} - 2.722{Instrumentalness}
\\
 + 0.354{Loudness} + 3.278{Speechiness} - 1.445{Valence}
$$ 

## Mode 1 popularity

### Full Logistic Model

fitting logistic model using combo of train/valid/test, finding optimal model using training data.
```{r, warning = FALSE}
# Fit logistic model on training data
v.logit.model1 <- glm(popular ~ ., family=binomial(link='logit'),data= logit.train1)

#search for best cutoff
out1 <- opt.cut.func(v.logit.model1, logit.valid1)
opt.cut.plot(out1)
out1$cutoff[which.min(out1$ssdiff.vec)]
v.opt.cut1 <- out1$cutoff[which.max(out1$kappa.vec)]
v.opt.cut1
```



Fit final model (combo of train and validation)
```{r}
v.model.final1 <-  glm(popular ~ ., data=all.train1, family=binomial(link='logit'))
summary(v.model.final1)
```


Full Logistic Model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 3.198 - 0.008{Duration} - 0.747{Acousticness} + 0.935{Danceability} - 3.457{Energy} - 12.325{Instrumentalness}
\\
+ 0.134{Key1} + 0.070{Key2} + 0.586{Key3} - 0.159{Key4} + 0.031{Key5} + 0.052{Key6} + 0.174{Key7} + 0.344{Key8} + 0.275{Key9}
\\
- 0.385{Key10} - 0.071{Key11} + 0.330{Loudness} + 4.734{Speechiness} + 0.003{Tempo} - 1.386{Valence}
$$

Variables Duration, Acousticness, Danceability, Energy, Key3 (D#/E-flat Major), Key8 (G#/A-flat Major), Loudness, Speechiness, and Valence are significant variables in the model.


predict on test using 0.5 cutoff 
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > 0.5, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

predict on test using optimal cutoff (kappa), 0.15
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > v.opt.cut1, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```


predict on test using optimal cutoff (sensitivity and specificity), 0.11
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > 0.11, 1, 0) # using cutoff = 0.11
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```



### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
meh <- capture.output(step1_kcv <- train(popular ~ ., data = logit.train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step1_kcv <- step1_kcv$finalModel
step1_kcv
summary(step1_kcv)
```

```{r, warning = FALSE}
#search for best cutoff
kcv.out1 <- opt.cut.func(step1_kcv, key.dummy(logit.valid1))
opt.cut.plot(kcv.out1)
kcv.out1$cutoff[which.min(kcv.out1$ssdiff.vec)]
kcv.out1.cut1 <- kcv.out1$cutoff[which.max(kcv.out1$kappa.vec)]
kcv.out1.cut1
```

Fit final model (combo of train and validation)
```{r}
set.seed(123)
finalmeh <- capture.output(step1_kcv.final <- train(popular ~ ., data = all.train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step1_kcv.final <- step1_kcv.final$finalModel
step1_kcv.final
summary(step1_kcv.final)
```



Stepwise 10 Fold Cross Validation:

Compared to the full Logistic model, the stepwise model does not include the variables Key1, Key2, Key4-Key7, Key9 and Key11.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 3.280 - 0.008{Duration} - 0.777{Acousticness} + 0.976{Danceability} - 3.478{Energy} - 12.106{Instrumentalness} 
\\
 + 0.513{Key3} - 0.267{Key8} -0.460{Key10} + 0.333{Loudness} + 4.790{Speechiness} + 0.003{Tempo} - 1.411{Valence}
\\
$$

Variables Duration, Acousticness, Danceability, Energy, Key3 (D#/E-flat Major), Loudness, Speechiness, and Valence are significant variables in the model.



predict on test using 0.5 cutoff 
```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Using cutoff 0.13 which yields optimal kappa
```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > kcv.out1.cut1, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Using cut off 0.11 which yields the best balance between sensitivity and specificity.
```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > 0.11, 1, 0) # using cutoff = 0.11
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```



### Variable Selection: All possible regression

```{r}
set.seed(123)
glmulti.out1 <- glmulti(popular ~ ., data = logit.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
```

view summary of top model
```{r}
summary(glmulti.out1@objects[[1]])
```

Store model
```{r}
allreg.logit1 <- glmulti.out1@objects[[1]]
```

```{r, warning = FALSE}
#search for best cutoff
allreg.out1 <- opt.cut.func(allreg.logit1, logit.valid1)
opt.cut.plot(allreg.out1)
allreg.out1$cutoff[which.min(allreg.out1$ssdiff.vec)]
allreg.out1.cut1 <- allreg.out1$cutoff[which.max(allreg.out1$kappa.vec)]
allreg.out1.cut1
```

fit final model to combo of training and validation
```{r}
set.seed(123)
glmulti.out1 <- glmulti(popular ~ ., data = all.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
summary(glmulti.out1@objects[[1]])
```


All Possible Regression
Compared to the full model, the All subsets regression model does not include any of the Key dummy variables.

$$

log(\frac{\hat \pi}{1 - \hat \pi}) = 3.289 - 0.008{Duration} - 0.750{Acousticness} + 0.942{Danceability} - 3.483{Energy} - 11.929{Instrumentalness}
\\
 + 0.333{Loudness} + 4.822{Speechiness} + 0.003{Tempo} - 1.406{Valence}

$$ 

Variables Duration, Acousticness, Danceability, Energy, Loudness, Speechiness, and Valence are significant in the model

store final model
```{r}
allreg.logit1.final <- glmulti.out1@objects[[1]]
```

Predictions with 0.5 as the cut off
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > 0.5, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Predictions where cut off is the best kappa, 0.16
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > allreg.out1.cut1, 1, 0) # using optimal cutoff 0.16
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Predictions where cut off is the best balance of sensitivity and specificity, 0.11
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > 0.11, 1, 0) # using optimal cutoff  0.11
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```





### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
set.seed(123)
ridge1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1$bestTune
ridge1.model <- coef(ridge1$finalModel, ridge1$bestTune$lambda)
ridge1.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
ridge1.out <- reg.opt.cut.func(ridge1, logit.valid1)
opt.cut.plot(ridge1.out)
# cut off by kappa
ridge1.out$cutoff[which.max(ridge1.out$kappa.vec)]
ridge1.out$cutoff[which.min(ridge1.out$ssdiff.vec)]
```

create final model
```{r}
set.seed(123)
ridge1 <- train(popular ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1$bestTune
ridge1.model <- coef(ridge1$finalModel, ridge1$bestTune$lambda)
ridge1.model
```

Ridge 10 Fold Cross Validation
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.031 - 0.003{Duration} - 0.103{Acousticness} + 0.159{Danceability} - 0.270{Energy} - 0.817{Instrumentalness}
\\
+ 0.027{Key1} + 0.007{Key2} + 0.266{Key3} - 0.114{Key4} - 0.025{Key5} - 0.032{Key6} + 0.078{Key7} + 0.138{Key8} + 0.130{Key9}
\\
- 0.214{Key10} - 0.064{Key11} + 0.065{Loudness} + 2.178{Speechiness} + 0.002{Tempo} - 0.474{Valence}
$$



predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.12 corresponding to optimal kappa
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.12, 1, 0) # using cutoff = 0.12
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.11, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```



### Regularized Regression: Lasso 10 fold Cross Validation (returns with a model of just the intercept)
```{r}
lasso1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso1$bestTune
# best coefficient
lasso1.model <- coef(lasso1$finalModel, lasso1$bestTune$lambda)
lasso1.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
lasso1.out <- reg.opt.cut.func(lasso1, logit.valid1)
opt.cut.plot(lasso1.out)
# cut off by kappa
lasso1.out$cutoff[which.max(lasso1.out$kappa.vec)]
lasso1.out$cutoff[which.min(lasso1.out$ssdiff.vec)]

```


```{r}
lasso1.final <- train(popular ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso1.final$bestTune
# best coefficient
lasso1.model.final <- coef(lasso1.final$finalModel, lasso1.final$bestTune$lambda)
lasso1.model.final
```

predict and evaluate on the test set where cutoff is at 0.5
```{r, warning = FALSE}
prob.lasso1 <- predict(lasso1.final, s = lasso1.final$bestTune, logit.test1, type = "prob")
pred.lasso1 <- ifelse(prob.lasso1[,2] > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.lasso1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11
```{r, warning = FALSE}
prob.lasso1 <- predict(lasso1.final, s = lasso1.final$bestTune, logit.test1, type = "prob")
pred.lasso1 <- ifelse(prob.lasso1[,2] > 0.11, 1, 0) # using cutoff = 0.11
confusionMatrix(as.factor(pred.lasso1), as.factor(logit.test1$popular), 
                positive = "1")
```


### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
set.seed(123)
enet1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1$bestTune
# best coefficient
enet1.model <- coef(enet1$finalModel, enet1$bestTune$lambda)
enet1.model
```

search for best cutoff with validation set
```{r, warning = FALSE}
enet1.out <- reg.opt.cut.func(enet1, logit.valid1)
opt.cut.plot(enet1.out)
# cut off by kappa
enet1.out$cutoff[which.max(enet1.out$kappa.vec)]
enet1.out$cutoff[which.min(enet1.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
set.seed(123)
enet1 <- train(popular ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1$bestTune
# best coefficient
enet1.model <- coef(enet1$finalModel, enet1$bestTune$lambda)
enet1.model
```


Elastic Net 10 Cross Validation
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.031 - 0.003{Duration} - 0.103{Acousticness} + 0.159{Danceability} - 0.270{Energy} - 0.817{Instrumentalness}
\\
+ 0.027{Key1} + 0.007{Key2} + 0.266{Key3} - 0.114{Key4} - 0.025{Key5} - 0.032{Key6} + 0.078{Key7} + 0.138{Key 8}
\\ 
+ 0.130{Key9} - 0.214{Key10} - 0.064{Key11} + 0.065{Loudness} + 2.178{Speechiness} + 0.002{Tempo} - 0.474{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.12 corresponding to optimal kappa
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.12, 1, 0) # using cutoff = 0.12
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.11, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```


### Decide on the best logistic model for Mode 1 




Diagnostic measures
```{r}
results.logit1 <- data.frame(model = c("Full", "Step 10CV", "All Subsets", "Ridge 10CV", "Elastic Net 10CV"),
                             cutoff = c(0.11, 0.11, 0.11, 0.11, 0.11),
                             AIC = c(4131.1, 4121, 4127.6, NA, NA),
                             Accuracy =c(0.6167, 0.6013, 0.6022, 0.5758, 0.5758),
                             Kappa = c(0.1018, 0.0857, 0.0893, 0.0962, 0.0962),
                             Sensitivity = c(0.61947, 0.60177, 0.61062, 0.68142, 0.68142),
                             Specificity = c(0.61640, 0.60121, 0.60121, 0.56377, 0.56377))
results.logit1
```

The Full logistic model yields the best performance in Accuracy and optimal balance between sensitivity and specificity.

Full Logistic Model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 3.198 - 0.008{Duration} - 0.747{Acousticness} + 0.935{Danceability} - 3.457{Energy} - 12.325{Instrumentalness}
\\
+ 0.134{Key1} + 0.070{Key2} + 0.586{Key3} - 0.159{Key4} + 0.031{Key5} + 0.052{Key6} + 0.174{Key7} + 0.344{Key8} + 0.275{Key9}
\\
\\
\\
- 0.385{Key10} - 0.071{Key11} + 0.330{Loudness} + 4.734{Speechiness} + 0.003{Tempo} - 1.386{Valence}
$$

Variables Duration, Acousticness, Danceability, Energy, Key3 (D#/E-flat Major), Key8 (G#/A-flat Major), Loudness, Speechiness, and Valence are significant variables in the model.