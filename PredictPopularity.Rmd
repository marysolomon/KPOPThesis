---
title: "Predict Popularity"
author: "Mary Solomon"
date: "2/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(QuantPsyc)#for multivariate normality function
library(data.table)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales)
library(caret)
library(leaps) # for all possible regressions approach LINEAR
library(glmulti)# for all possible regressions approach LOGISTIC
```


global functions (convert key to dummy)
```{r}
key.dummy <- function(x){
  x <- x %>% mutate(key1 = ifelse(x$key == 1, 1, 0),
              key2 = ifelse(x$key == 2, 1, 0),
              key3 = ifelse(x$key == 3, 1, 0),
              key4 = ifelse(x$key == 4, 1, 0),
              key5 = ifelse(x$key == 5, 1, 0),
              key6 = ifelse(x$key == 6, 1, 0),
              key7 = ifelse(x$key == 7, 1, 0),
              key8 = ifelse(x$key == 8, 1, 0),
              key9 = ifelse(x$key == 9, 1, 0),
              key10 = ifelse(x$key == 10, 1, 0),
              key11 = ifelse(x$key == 11, 1, 0)) %>% select(-key)
    
  x
}

#train/valid/test function from data mining class:
partition.3 <- function(data, prop.train, prop.val){
  # select a random sample of size = prop.train % of total records
  selected1 <- sample(1:nrow(data), round(nrow(data)*prop.train), replace = FALSE) 
  # create training data which has prop.train % of total records
  data.train <- data[selected1,]
  # select a random sample of size = prop.val % of the total records
  rest <- setdiff(1:nrow(data), selected1)
  selected2 <- sample(rest, round(nrow(data)*prop.val), replace = FALSE) 
  # create validation data which has prop.val % of total records
  data.val <- data[selected2,]
  # create testing data with the remaining records
  data.test <- data[setdiff(rest, selected2),]
  return(list(data.train=data.train, data.test=data.test, data.val=data.val))
}

#creates plot for optimal cut off value
opt.cut.func <- function(model, data){
  # create a vector for cutoff values
  cutoff <- seq(0, 1, 0.01)

  # create three empty vectors of same length
  sensitivity.vec <- rep(NA, length(cutoff))
  specificity.vec <- rep(NA, length(cutoff))
  ssdiff.vec <- rep(NA, length(cutoff))
  kappa.vec <- rep(NA, length(cutoff))

# For loop.
  for(i in 1:length(cutoff)){
    pred.prob.val <- predict(model, newdata = data, type = "response")
    pred.y.val <- as.factor(ifelse(pred.prob.val > cutoff[i], 1, 0)) 
    # warning messages galore because the probability of a value actually being popular is SO LOW
    c <- confusionMatrix(pred.y.val, data$popular, 
                        positive = "1")
    sensitivity.vec[i] <- c$byClass["Sensitivity"]
    specificity.vec[i] <- c$byClass["Specificity"]
    ssdiff.vec[i] <- abs(sensitivity.vec[i] - specificity.vec[i])
    kappa.vec[i] <- c$overall["Kappa"]
  }
  return(list(cutoff = cutoff, sensitivity.vec = sensitivity.vec, specificity.vec = specificity.vec, ssdiff.vec = ssdiff.vec, kappa.vec = kappa.vec))
}

reg.opt.cut.func <-  function(model, data){
  cutoff <- seq(0, 1, 0.01)

  # create three empty vectors of same length
  sensitivity.vec <- rep(NA, length(cutoff))
  specificity.vec <- rep(NA, length(cutoff))
  ssdiff.vec <- rep(NA, length(cutoff))
  kappa.vec <- rep(NA, length(cutoff))

  # For loop.
  for(i in 1:length(cutoff)){
    pred.prob.val <- predict(model, s=model$bestTune, data, type = "prob")
    pred.y.val <- as.factor(ifelse(pred.prob.val[,2] > cutoff[i], 1, 0)) 
    # warning messages galore because the probability of a value actually being popular is SO LOW
    c <- confusionMatrix(pred.y.val, data$popular, 
                        positive = "1")
    sensitivity.vec[i] <- c$byClass["Sensitivity"]
    specificity.vec[i] <- c$byClass["Specificity"]
    ssdiff.vec[i] <- abs(sensitivity.vec[i] - specificity.vec[i])
    kappa.vec[i] <- c$overall["Kappa"]
  }
  return(list(cutoff = cutoff, sensitivity.vec = sensitivity.vec, specificity.vec = specificity.vec, ssdiff.vec = ssdiff.vec, kappa.vec = kappa.vec))
}

opt.cut.plot <- function(opt.out){
  plot(opt.out$cutoff, opt.out$sensitivity.vec,xlab = "cutoff", type = "l", col = "blue")
  lines(opt.out$cutoff, opt.out$specificity.vec, type = "l", col = "green")
  lines(opt.out$cutoff, opt.out$kappa.vec, type = "l", col = "red")
  legend( x="right", legend=c("Sensitivity","Specificity", "Kappa"),
        col=c("blue","green","red"), lty = 1, lwd=1)
  
}
```


data :D
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               release_date = as.POSIXct(release_date, format = "%m/%d/%y"),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)),
               popular = factor(ifelse(popularity >=50, 1, 0)))
```

understanding popularity
```{r}
hist(data$popularity)
summary(data$popularity)
```

heavily skewed right.


The square root transformation makes it more normal. This will help to meet the multiple linear regression assumptions.
```{r}
hist(data$popularity^0.5)
```


Classification of 
```{r}
table(data$popular)/nrow(data)
```

if classifying a song as popular when it's score is greater than 50, only ~12% of the data is considered a popular song.

```
kpop <- dplyr::select(data, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, time_signature, tempo, valence)
```

General Assumptions:
continuous response: popularity score ranging from 0 - 100.
mix of categorical and continuous response.
the distribution of the variables are not normal, we will check for normality of error terms where appropriate.

Goal: create model for predicting popularity scores


# Multiple Linear Model?
select just audio features
```{r}
kpop <- select(data, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% filter(mode == 0)%>% select(-mode)
kpop1 <- kpop %>% filter(mode == 1) %>% select(-mode)

### Kpop mode 0 train and test
smpl.size0 <- floor(0.75*nrow(kpop0))
set.seed(123)
smpl0 <- sample(nrow(kpop0), smpl.size0, replace = FALSE)
train0 <- kpop0[smpl0,]
test0 <- kpop0[-smpl0,]

### Kpop mode 1 train and test
smpl.size1 <- floor(0.75*nrow(kpop1))
set.seed(123)
smpl1 <- sample(nrow(kpop1), smpl.size1, replace = FALSE)
train1 <- kpop1[smpl1,]
test1 <- kpop1[-smpl1,]
```



## For Songs with Mode 0

### Full Multiple Linear Regression
fit a multiple linear regression model
```{r}
ml0 <- lm(popularity ~. , data = train0)
summary(ml0)
```



```{r}
plot(ml0)
```

no or little multicollinearity

no autocorrelation 

no homoscedasticity.

Try again with tranformation to make popularity normal:(to the squareroot)
```{r}
ml0.sqrt <- lm(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5))
summary(ml0.sqrt)
```

assumption checking and diagnostics
```{r}
plot(ml0.sqrt)
```

prediction on test data
```{r}
# prediction on test data
yhat.mlr = predict(ml0.sqrt, newdata = test0 %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.mlr <- yhat.mlr - test0$popularity^0.5
rmse.mlr <- sqrt(mean(error.mlr^2))
data.frame(
  RMSE = RMSE(yhat.mlr, test0$popularity^0.5),
  R2 = R2(yhat.mlr, test0$popularity^0.5)
)
```

Much improved!




### Variable Selection: Stepwise 10 fold cross validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5),  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv)
mlr.step_kcv$finalModel 
```

prediction on test data
```{r}
# prediction on test data
yhat.step_kcv = predict(mlr.step_kcv$finalModel, newdata=key.dummy(test0) %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.step_kcv <- yhat.step_kcv - test0$popularity^0.5
rmse.step_kcv <- sqrt(mean(error.step_kcv^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv, test0$popularity^0.5),
  R2 = R2(yhat.step_kcv, test0$popularity^0.5)
)
```

Both of the models have RMSE scores of around 2. This isn't terrible since the range of the transformed popularity scores is 0-10.

### Regularized Regression: Ridge
```{r}
lm.ridge0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge0$bestTune
# best coefficient
lm.ridge0.model <- coef(lm.ridge0$finalModel, lm.ridge0$bestTune$lambda)
lm.ridge0.model
```

```{r}
# prediction on test data
yhat.ridge0 = predict(lm.ridge0, s=lm.ridge0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge0, test0$popularity^0.5),
  R2 = R2(yhat.ridge0, test0$popularity^0.5)
)
```

### Regularized Regression: Lasso
```{r}
lm.lasso0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso0$bestTune
# best coefficient
lm.lasso0.model <- coef(lm.lasso0$finalModel, lm.lasso0$bestTune$lambda)
lm.lasso0.model
```


```{r}
# prediction on test data
yhat.lasso0 = predict(lm.lasso0, s=lm.lasso0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.lasso0 <- yhat.lasso0 - test0$popularity^0.5
rmse.lasso0 <- sqrt(mean(error.lasso0^2))
data.frame(
  RMSE = RMSE(yhat.lasso0, test0$popularity^0.5),
  R2 = R2(yhat.lasso0, test0$popularity^0.5)
)
```

### Regularized Regression: Elastic Net
```
lm.enet0 <- train(popularity ~. , data = train0 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lm.enet0$bestTune
# best coefficient
lm.enet0.model <- coef(lm.enet0$finalModel, lm.enet0$bestTune$lambda)
lm.enet0.model
```

```
# prediction on test data
yhat.enet0 = predict(lm.enet0, s=lm.enet0$bestTune,newdata=test0%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.enet0 <- yhat.enet0 - test0$popularity^0.5
rmse.enet0 <- sqrt(mean(error.enet0^2))
data.frame(
  RMSE = RMSE(yhat.enet0, test0$popularity^0.5),
  R2 = R2(yhat.enet0, test0$popularity^0.5)
)
```

## For songs with Mode 1

### Full Multiple linear model
```{r}
ml1.sqrt <- lm(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5))
summary(ml1.sqrt)
```

prediction on test data
```{r}
# prediction on test data
yhat.mlr = predict(ml1.sqrt, newdata = test1 %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.mlr <- yhat.mlr - test1$popularity^0.5
rmse.mlr <- sqrt(mean(error.mlr^2))
data.frame(
  RMSE = RMSE(yhat.mlr, test1$popularity^0.5),
  R2 = R2(yhat.mlr, test1$popularity^0.5)
)
```


### Variable Selection: Stepwise 10 fold cross validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5),  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv)
mlr.step_kcv$finalModel 
```

prediction on test data
```{r}
# prediction on test data
yhat.step_kcv = predict(mlr.step_kcv$finalModel, newdata=key.dummy(test1) %>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.step_kcv <- yhat.step_kcv - test1$popularity^0.5
rmse.step_kcv <- sqrt(mean(error.step_kcv^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv, test1$popularity^0.5),
  R2 = R2(yhat.step_kcv, test1$popularity^0.5)
)
```

### Regularized Regression: Ridge
```{r}
lm.ridge1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge1$bestTune
# best coefficient
lm.ridge1.model <- coef(lm.ridge1$finalModel, lm.ridge1$bestTune$lambda)
lm.ridge1.model
```

```{r}
# prediction on test data
yhat.ridge1 = predict(lm.ridge1, s=lm.ridge1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge1, test1$popularity^0.5),
  R2 = R2(yhat.ridge1, test1$popularity^0.5)
)
```


### Regularized Regression: Lasso
```{r}
lm.lasso1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso1$bestTune
# best coefficient
lm.lasso1.model <- coef(lm.lasso1$finalModel, lm.lasso1$bestTune$lambda)
lm.lasso1.model
```


```{r}
# prediction on test data
yhat.lasso1 = predict(lm.lasso1, s=lm.lasso1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.lasso1 <- yhat.lasso1 - test1$popularity^0.5
rmse.lasso1 <- sqrt(mean(error.lasso1^2))
data.frame(
  RMSE = RMSE(yhat.lasso1, test1$popularity^0.5),
  R2 = R2(yhat.lasso1, test1$popularity^0.5)
)
```

### Regularized Regression: Elastic Net
```
lm.enet1 <- train(popularity ~. , data = train1 %>% mutate(popularity = popularity^0.5), method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lm.enet1$bestTune
# best coefficient
lm.enet1.model <- coef(lm.enet1$finalModel, lm.enet1$bestTune$lambda)
lm.enet1.model
```

```
# prediction on test data
yhat.enet1 = predict(lm.enet1, s=lm.enet1$bestTune,newdata=test1%>% mutate(popularity = popularity^0.5))
# RMSE for test data
error.enet1 <- yhat.enet1 - test1$popularity^0.5
rmse.enet1 <- sqrt(mean(error.enet1^2))
data.frame(
  RMSE = RMSE(yhat.enet1, test1$popularity^0.5),
  R2 = R2(yhat.enet1, test1$popularity^0.5)
)
```


# Logistic (classification approach)

Use 70% train, 15% validation, 15% test,  to use validation for finding optimal cutoff value.
```{r}
kpop.logit <- select(data, popular, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
logit.kpop0 <- kpop.logit %>% filter(mode == 0)%>% select(-mode)
logit.kpop1 <- kpop.logit %>% filter(mode == 1) %>% select(-mode)

### Kpop mode 0 train and test
# smpl.size0 <- floor(0.75*nrow(logit.kpop0))
# set.seed(123)
# smpl0 <- sample(nrow(logit.kpop0), smpl.size0, replace = FALSE)
# og.logit.train0 <- logit.kpop0[smpl0,]
# og.logit.test0 <- logit.kpop0[-smpl0,]

p3 <- partition.3(logit.kpop0, 0.70, 0.15)
logit.train0 <- p3$data.train
logit.valid0 <- p3$data.val
logit.test0 <- p3$data.test
all.train0 <- rbind(logit.train0, logit.valid0)

# ### Kpop mode 1 train and test
# smpl.size1 <- floor(0.75*nrow(logit.kpop1))
# set.seed(123)
# smpl1 <- sample(nrow(logit.kpop1), smpl.size1, replace = FALSE)
# logit.train1 <- logit.kpop1[smpl1,]
# logit.test1 <- logit.kpop1[-smpl1,]

p3 <- partition.3(logit.kpop1, 0.70, 0.15)
logit.train1 <- p3$data.train
logit.valid1 <- p3$data.val
logit.test1 <- p3$data.test
all.train1 <- rbind(logit.train1, logit.valid1)
```


## Mode 0 popularity

### Full Logistic Model: train/test

fitting logistic model using combo of train/test, finding optimal model using training data.
```
# Fit logistic model on training data
og.logit.model0 <- glm(popular ~ ., family=binomial(link='logit'),data= og.logit.train0)

# create a vector for cutoff values
cutoff <- seq(0, 1, 0.05)

# create three empty vectors of same length
sensitivity.vec <- rep(NA, length(cutoff))
specificity.vec <- rep(NA, length(cutoff))
kappa.vec <- rep(NA, length(cutoff))

# For loop.
for(i in 1:length(cutoff)){
  pred.prob.val <- predict(og.logit.model0, newdata = og.logit.train0, type = "response")
  pred.y.val <- as.factor(ifelse(pred.prob.val > cutoff[i], 1, 0)) 
  # warning messages galore because the probability of a value actually being popular is SO LOW
  c <- confusionMatrix(pred.y.val, og.logit.train0$popular, 
                       positive = "1")
  sensitivity.vec[i] <- c$byClass["Sensitivity"]
  specificity.vec[i] <- c$byClass["Specificity"]
  kappa.vec[i] <- c$overall["Kappa"]
}

plot(cutoff, sensitivity.vec,xlab = "cutoff", type = "l", col = "blue")
lines(cutoff, specificity.vec, type = "l", col = "green")
lines(cutoff, kappa.vec, type = "l", col = "red")
legend( x="right", legend=c("Sensitivity","Specificity", "Kappa"),
        col=c("blue","green","red"), lty = 1, lwd=1)

#ideal cut off value
opt.cut <- cutoff[which.max(kappa.vec)]
opt.cut
```


Evaluate performance on test

Using just og cut off
```
og.prob.test <- predict(og.logit.model0, newdata = og.logit.test0, type = "response")
og.pred.test <- ifelse(og.prob.test > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(og.pred.test), as.factor(og.logit.test0$popular), positive = "1")
```

Using optimal cutoff.
```
og.prob.test <- predict(og.logit.model0, newdata = og.logit.test0, type = "response")
og.pred.test <- ifelse(og.prob.test > opt.cut, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(og.pred.test), as.factor(og.logit.test0$popular), positive = "1")
```



### Full Logistic Model: train/valid/test

fitting logistic model using combo of train/valid/test, finding optimal model using training data.
```{r}
# Fit logistic model on training data
v.logit.model0 <- glm(popular ~ ., family=binomial(link='logit'),data= logit.train0)

#search for best cutoff
out0 <- opt.cut.func(v.logit.model0, logit.valid0)
opt.cut.plot(out0)
out0$cutoff[which.min(out0$ssdiff.vec)]
v.opt.cut0 <- out0$cutoff[which.max(out0$kappa.vec)]
v.opt.cut0
```

Fit final model (combo of train and validation)
```{r}
v.model.final <-  glm(popular ~ ., data=all.train0, family=binomial(link='logit'))
```

predict on test using 0.5 cutoff 
```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > 0.5, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```



predict on optimal cutoff (0.16)
```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > v.opt.cut0, 1, 0) # using cutoff = 0.16
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```


```{r}
v.prob.test <- predict(v.logit.model0, newdata = logit.test0, type = "response")
v.pred.test <- ifelse(v.prob.test > 0.13, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(v.pred.test), as.factor(logit.test0$popular), positive = "1")
```


### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
meh <- capture.output(step0_kcv <- train(popular ~ ., data = logit.train0, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step0_kcv <- step0_kcv$finalModel
step0_kcv
```


```
kcv.prob.test0 <- predict(step0_kcv, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

Find optimal cut off value:
```{r}
#search for best cutoff
kcv.out0 <- opt.cut.func(step0_kcv, key.dummy(logit.valid0))
opt.cut.plot(kcv.out0)
kcv.out0$cutoff[which.min(kcv.out0$ssdiff.vec)]
kcv.out0.cut0 <- kcv.out0$cutoff[which.max(kcv.out0$kappa.vec)]
kcv.out0.cut0
```

Fit final model (combo of train and validation)
```{r}
finalmeh <- capture.output(step0_kcv.final0 <- train(popular ~ ., data = all.train0, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
step0_kcv.final0 <- step0_kcv.final0$finalModel
step0_kcv.final0
```

predict on test using 0.5 cutoff 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

predict on test using 0.16 cutoff 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.16, 1, 0) # using cutoff = 0.16
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```

predict on test using 0.13 cutoff 
```{r}
kcv.prob.test0 <- predict(step0_kcv.final0, newdata = key.dummy(logit.test0), type = "response")
kcv.pred.test0 <- ifelse(kcv.prob.test0 > 0.13, 1, 0) # using cutoff = 0.16
confusionMatrix(as.factor(kcv.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


### Variable Selection: All possible regression

```{r}
glmulti.out0 <- glmulti(popular ~ ., data = logit.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out0@formulas
```

view summary of top model
```{r}
summary(glmulti.out0@objects[[1]])
```


Store model
```{r}
allreg.logit0 <- glmulti.out0@objects[[1]]
```

```{r}
#search for best cutoff
allreg.out0 <- opt.cut.func(allreg.logit0, logit.valid0)
opt.cut.plot(allreg.out0)
allreg.out0$cutoff[which.min(allreg.out0$ssdiff.vec)]
allreg.out0.cut0 <- allreg.out0$cutoff[which.max(allreg.out0$kappa.vec)]
allreg.out0.cut0
```

fit final model to combo of training and validation
```{r}
glmulti.out0 <- glmulti(popular ~ ., data = all.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
summary(glmulti.out0@objects[[1]])
```

store final model
```{r}
allreg.logit0.final <- glmulti.out0@objects[[1]]
```

Predictions with 0.5 as the cut off
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > 0.5, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


Predictions where cut off is the best kappa
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > allreg.out0.cut0, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


Predictions where cut off is the best balance of sensitivity and specificity
```{r}
allreg.prob.test0 <- predict(allreg.logit0.final, newdata = logit.test0, type = "response")
allreg.pred.test0 <- ifelse(allreg.prob.test0 > 0.14, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test0), as.factor(logit.test0$popular), positive = "1")
```


### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
ridge0 <- train(popular ~ ., data = logit.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0$bestTune
ridge0.model <- coef(ridge0$finalModel, ridge0$bestTune$lambda)
ridge0.model
```

Search for best cutoff using validation set
```{r}
ridge0.out <- reg.opt.cut.func(ridge0, logit.valid0)
opt.cut.plot(ridge0.out)
# cut off by kappa
ridge0.out$cutoff[which.max(ridge0.out$kappa.vec)]
ridge0.out$cutoff[which.min(ridge0.out$ssdiff.vec)]
```


create final model
```{r}
ridge0 <- train(popular ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0$bestTune
ridge0.model <- coef(ridge0$finalModel, ridge0$bestTune$lambda)
ridge0.model
```

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge0 <- predict(ridge0, s = ridge0$bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.12 corresponding to optimal kappa
```{r}
prob.ridg0 <- predict(ridge0, s = ridge0bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.12, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```


predict and evaluate on the test set where cutoff is at 0.14 corresponding to optimal sensitivity and specificity balance
```{r}
prob.ridg0 <- predict(ridge0, s = ridge0bestTune, logit.test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.14, 1, 0) # using cutoff = 0.14
confusionMatrix(as.factor(pred.ridge0), as.factor(logit.test0$popular), 
                positive = "1")
```

### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
enet0 <- train(popular ~ ., data = logit.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0$bestTune
# best coefficient
enet0.model <- coef(enet0$finalModel, enet0$bestTune$lambda)
enet0.model
```

search for best cutoff with validation set
```{r}
enet0.out <- reg.opt.cut.func(enet0, logit.valid0)
opt.cut.plot(enet0.out)
# cut off by kappa
enet0.out$cutoff[which.max(enet0.out$kappa.vec)]
enet0.out$cutoff[which.min(enet0.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
enet0 <- train(popular ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0$bestTune
# best coefficient
enet0.model <- coef(enet0$finalModel, enet0$bestTune$lambda)
enet0.model
```

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.12 corresponding to optimal kappa
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.12, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.14 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet0 <- predict(enet0, s = enet0$bestTune, logit.test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.14, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.enet0), as.factor(logit.test0$popular), 
                positive = "1")
```



## Mode 1 popularity

### Full Logistic Model: train/valid/test

fitting logistic model using combo of train/valid/test, finding optimal model using training data.
```{r}
# Fit logistic model on training data
v.logit.model1 <- glm(popular ~ ., family=binomial(link='logit'),data= logit.train1)

#search for best cutoff
out1 <- opt.cut.func(v.logit.model1, logit.valid1)
opt.cut.plot(out1)
out1$cutoff[which.min(out1$ssdiff.vec)]
v.opt.cut1 <- out1$cutoff[which.max(out1$kappa.vec)]
v.opt.cut1
```



Fit final model (combo of train and validation)
```{r}
v.model.final1 <-  glm(popular ~ ., data=all.train1, family=binomial(link='logit'))
```

predict on test using 0.5 cutoff 
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > 0.5, 1, 0) # using cutoff = 0.15
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

predict on test using optimal cutoff (kappa)
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > v.opt.cut1, 1, 0) # using cutoff = 0.16
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```


predict on test using optimal cutoff (sensitivity and specificity)
```{r}
v.prob.test1 <- predict(v.logit.model1, newdata = logit.test1, type = "response")
v.pred.test1 <- ifelse(v.prob.test1 > 0.11, 1, 0) # using cutoff = 0.11
confusionMatrix(as.factor(v.pred.test1), as.factor(logit.test1$popular), positive = "1")
```



### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
meh <- capture.output(step1_kcv <- train(popular ~ ., data = logit.train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step1_kcv <- step1_kcv$finalModel
step1_kcv
```

```{r}
#search for best cutoff
kcv.out1 <- opt.cut.func(step1_kcv, key.dummy(logit.valid1))
opt.cut.plot(kcv.out1)
kcv.out1$cutoff[which.min(kcv.out1$ssdiff.vec)]
kcv.out1.cut1 <- kcv.out1$cutoff[which.max(kcv.out1$kappa.vec)]
kcv.out1.cut1
```

Fit final model (combo of train and validation)
```{r}
finalmeh <- capture.output(step1_kcv.final <- train(popular ~ ., data = all.train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE))
```

```{r}
step1_kcv.final <- step1_kcv.final$finalModel
step1_kcv.final
```



predict on test using 0.5 cutoff 
```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```


```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > kcv.out1.cut1, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

```{r}
kcv.prob.test1 <- predict(step1_kcv.final, newdata = key.dummy(logit.test1), type = "response")
kcv.pred.test1 <- ifelse(kcv.prob.test1 > 0.11, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(kcv.pred.test1), as.factor(logit.test1$popular), positive = "1")
```



### Variable Selection: All possible regression

```{r}
glmulti.out1 <- glmulti(popular ~ ., data = logit.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
```

view summary of top model
```{r}
summary(glmulti.out1@objects[[1]])
```

Store model
```{r}
allreg.logit1 <- glmulti.out1@objects[[1]]
```

```{r}
#search for best cutoff
allreg.out1 <- opt.cut.func(allreg.logit1, logit.valid1)
opt.cut.plot(allreg.out1)
allreg.out1$cutoff[which.min(allreg.out1$ssdiff.vec)]
allreg.out1.cut1 <- allreg.out1$cutoff[which.max(allreg.out1$kappa.vec)]
allreg.out1.cut1
```

fit final model to combo of training and validation
```{r}
glmulti.out1 <- glmulti(popular ~ ., data = all.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
#glmulti.out1@formulas
summary(glmulti.out1@objects[[1]])
```

store final model
```{r}
allreg.logit1.final <- glmulti.out1@objects[[1]]
```

Predictions with 0.5 as the cut off
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > 0.5, 1, 0) # using standard cutoff 0.5
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Predictions where cut off is the best kappa
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > allreg.out1.cut1, 1, 0) # using optimal cutoff 0.17
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```

Predictions where cut off is the best balance of sensitivity and specificity
```{r}
allreg.prob.test1 <- predict(allreg.logit1.final, newdata = logit.test1, type = "response")
allreg.pred.test1 <- ifelse(allreg.prob.test1 > 0.11, 1, 0) # using optimal cutoff 
confusionMatrix(as.factor(allreg.pred.test1), as.factor(logit.test1$popular), positive = "1")
```






### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
ridge1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1$bestTune
ridge1.model <- coef(ridge1$finalModel, ridge1$bestTune$lambda)
ridge1.model
```

Search for best cutoff using validation set
```{r}
ridge1.out <- reg.opt.cut.func(ridge1, logit.valid1)
opt.cut.plot(ridge1.out)
# cut off by kappa
ridge1.out$cutoff[which.max(ridge1.out$kappa.vec)]
ridge1.out$cutoff[which.min(ridge1.out$ssdiff.vec)]
```

create final model
```{r}
ridge1 <- train(popular ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1$bestTune
ridge1.model <- coef(ridge1$finalModel, ridge1$bestTune$lambda)
ridge1.model
```

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.13 corresponding to optimal kappa
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.13, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.ridge1 <- predict(ridge1, s = ridge1$bestTune, logit.test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.11, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.ridge1), as.factor(logit.test1$popular), 
                positive = "1")
```



### Regularized Regression: Lasso 10 fold Cross Validation (not kosher, returns with a model of just the intercept)
```
lasso1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso1$bestTune
# best coefficient
lasso1.model <- coef(lasso1$finalModel, lasso1$bestTune$lambda)
lasso1.model
```

Search for best cutoff using validation set
```
lasso1.out <- reg.opt.cut.func(lasso1, logit.valid1)
opt.cut.plot(lasso1.out)
# cut off by kappa
lasso1.out$cutoff[which.max(lasso1.out$kappa.vec)]
lasso1.out$cutoff[which.min(lasso1.out$ssdiff.vec)]

```


### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
enet1 <- train(popular ~ ., data = logit.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1$bestTune
# best coefficient
enet1.model <- coef(enet1$finalModel, enet1$bestTune$lambda)
enet1.model
```

search for best cutoff with validation set
```{r}
enet1.out <- reg.opt.cut.func(enet1, logit.valid1)
opt.cut.plot(enet1.out)
# cut off by kappa
enet1.out$cutoff[which.max(enet1.out$kappa.vec)]
enet1.out$cutoff[which.min(enet1.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
enet1 <- train(popular ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1$bestTune
# best coefficient
enet1.model <- coef(enet1$finalModel, enet1$bestTune$lambda)
enet1.model
```

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.13 corresponding to optimal kappa
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.13, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.11 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet1 <- predict(enet1, s = enet1$bestTune, logit.test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.11, 1, 0) # using cutoff = 0.13
confusionMatrix(as.factor(pred.enet1), as.factor(logit.test1$popular), 
                positive = "1")
```