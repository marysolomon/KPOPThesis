---
title: 'Linear Regression: Release Date'
author: "Mary Solomon"
date: "2/15/2021"
output: 
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(lubridate) #for working with time stamps :D
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales) #for formatting with percentage signs
library(nnet) #for multinomial regression
library(caret)
library(car) #for vif function and boxcox transformation
library(MASS) #boxcox transformation
library(leaps) # for all possible regressions approach LINEAR
library(moments) # for calculating skewness
```


Load the functions
```{r}
source("thesis_functions.R")
dhist.plot <- function(d, v, vname){
  ggplot(d, aes(x=v)) + 
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(d$v), sd = sd(d$v)), 
    lwd = 1, 
    col = 'red'
  ) +
  # xlab(vname) + ylab("Density") +
  # ggtitle(paste(vname, "Distribution")) +
  theme_bw()
}
```


set levels of ordinal data, and other variables to their appropriate dataypes. Did not adjust the datatypes for date/time since those will not be used in this portion
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               release_date = as.POSIXct(release_date, format = "%m/%d/%y"),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)))

#make month/year as continuous data in the form of number of months (~360)
ref.year = 1992 #we are using January 1992 as the reference
data$months = 12*(year(data$release_date) - ref.year) + month(data$release_date)
#View(select(data, Artist, song_name, release_date, months))
```

Testing how to transform back from a log function ... this works :D  
```{r}
data$months[1:10]
log.trans <- log(360-data$months);log.trans[1:10]
back.trans <- (exp(log.trans) - 360) / -1;back.trans[1:10]
```


# Assessing normality of the Response Variable: Month of release date.

### original months value

Summary of the data to gain an understanding of the range.
```{r}
summary(data$months)
```



check out distribution of the months 
```{r}
#dhist.plot(data, months, "Month Released")
ggplot(data, aes(x=months)) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(data$months), sd = sd(data$months)), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Month Released") + ylab("Density") +
  ggtitle(paste("Month Released", "Distribution")) +
  theme_bw()
```


```{r}
#produce normal qqplot to assess normality
qqnorm(data$months, pch = 1, frame = FALSE)
qqline(data$months, col = "red", lwd = 2)
#Skewness score
skewness(data$months)
```



try to find a transformation to normalize. As we can see the distribution of months is severely skewed to the left. 

### log transform for moderately negative skewed data
A common distribution towards normality for moderately skewed data is : $log_{10}(max(y+1) - y)$, since the max number of months is 349, the transformation would be : $log(350 - y)$

summary of the data under this transformation
```{r}
summary(log(350-data$months))
```

Already by looking at the 5 number summary it is evidence that there exists skewness isnce the median is much closer to the max than the min. 

```{r}
ggplot(data, aes(x=log(350-data$months))) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(log(350-data$months)), sd = sd(log(350-data$months))), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Month Released : log(350 - y))") + ylab("Density") +
  ggtitle(paste("Month Released : log(350 - y))", "Distribution")) +
  theme_bw()
```

```{r}
#produce normal qqplot to assess normality
qqnorm(log(350-data$months), pch = 1, frame = FALSE)
qqline(log(350-data$months), col = "red", lwd = 2)
#Skewness score
skewness(log(350-data$months))
```



The distribution looks more normal, but still has noticeable issues of skewness. Therefore, the transformation has not been successful in achieving normality as the observations deviate to much from the normal QQ line.


Since there still exists issues of skewness, we will tweak the standard transformation of negative skewness to this alternative: $log(360 - y)$

summary of the data under this transformation
```{r}
summary(log(360-data$months))
```
Notice that the maximum and minimum value are still very similar to before, however the minimum has increased with this change and now the median is at a more central location than before.

```{r}
ggplot(data, aes(x=log(360-data$months))) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(log(360-data$months)), sd = sd(log(360-data$months))), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Month Released : log(360 - y))") + ylab("Density") +
  ggtitle(paste("Month Released : log(360 - y))", "Distribution")) +
  theme_bw()
```


```{r}
#produce normal qqplot to assess normality
qqnorm(log(360-data$months), pch = 1, frame = FALSE)
qqline(log(360-data$months), col = "red", lwd = 2)
#calculate skewness
skewness(log(360-data$months))
```



The distribution of months has greatly improved and looks roughly normal. Furthermore, the normal qqplot shows great improvement 


### square root transform for moderately negative skewed data

```{r}
summary(sqrt(350-data$months))
```


Another common transformation for moderate skewed data is a square root approach: $\sqrt{max(y+1) - y}$ = $\sqrt{(350 - y)}$
```{r}
ggplot(data, aes(x=sqrt(350-data$months))) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(sqrt(350-data$months)), sd = sd(sqrt(350-data$months))), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Month Released : sqrt(350 - y))") + ylab("Density") +
  ggtitle(paste("Month Released : sqrt(350 - y))", "Distribution")) +
  theme_bw()
```

```{r}
#produce normal qqplot to assess normality
qqnorm(sqrt(350-data$months), pch = 1, frame = FALSE)
qqline(sqrt(350-data$months), col = "red", lwd = 2)
#calculate skewness
skewness(sqrt(350-data$months))
```

It is still somewhat positively skewed, but pretty good!



### Inverse transform for severely negative skewed data
Lastly, a common transformation for severely negatively skewed data takes an inverse approach: $\frac{1}{(max(y)+1) - y)}$ = $\frac{1}{(350 - y)}$
```{r}
ggplot(data, aes(x=1/(350-data$months))) +
  geom_histogram(aes(y = stat(density)), bins = 30, fill = "white", col = "black") +
  stat_function(
    fun = dnorm, 
    args = list(mean = mean(1/(350-data$months)), sd = sd(1/(350-data$months))), 
    lwd = 1, 
    col = 'red'
  ) +
  xlab("Month Released : 1/(350 - y))") + ylab("Density") +
  ggtitle(paste("Month Released : 1/(350 - y))", "Distribution")) +
  theme_bw()
```

This is a terrible idea. no.



# splitting into training and test data.
Create Training (75%) and Test data (25%) to train classification models on.
```{r}
kpop <- dplyr::select(data, months, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% dplyr::filter(mode == 0) %>% dplyr::select(-mode)
kpop1 <- kpop %>% dplyr::filter(mode == 1) %>% dplyr::select(-mode)

### Kpop mode 0 train and test
smpl.size0 <- floor(0.75*nrow(kpop0))
set.seed(123)
smpl0 <- sample(nrow(kpop0), smpl.size0, replace = FALSE)
train0 <- kpop0[smpl0,]
test0 <- kpop0[-smpl0,]

### Kpop mode 1 train and test
smpl.size1 <- floor(0.75*nrow(kpop1))
set.seed(123)
smpl1 <- sample(nrow(kpop1), smpl.size1, replace = FALSE)
train1 <- kpop1[smpl1,]
test1 <- kpop1[-smpl1,]
```



# Assessing performance of the transformations on Multiple linear regression for mode 0 songs
```{r}
ml0 <- lm(months ~. , data = train0)
summary(ml0)
```

check assumptions with diagnostic plots
```{r}
#par(mfrow = c(4,1))
plot(ml0)
```

 * Residuals vs. Fitted : Does not show a horizontal line. There is a noticeable pattern of a slight curve in the trend. 
 
 * Normal QQ: Residual points roughly follow the normal QQ line. The left side falls below the normal line below theoretical quantile -2. Otherwise, the distribution of the residuals appears to roughly follow a normal distribution.   
 
 *Scale-Location: Due to the clear decreasing line rather thana flat horizontal line of equally spread points, there is clear evidence of violation for homogeneity of the variance of the residuals. May need to run the model on a transformation of the outcome variable which is the months of song release.
 
 * Residuals vs Leverage: There are some points that exceed -3 and 3 standard deviations, so we sould remove such points.
 



```{r}
qqnorm(ml0$residuals)
qqline(ml0$residuals, col = "red")
```

checking for multicollinearity
```{r}
car::vif(ml0)
```


Overall, the current model does not meet the assumptions of the linear model. We need to make a transformation on the response variable to achieve normality of the residuals

```{r}
yhat.ml0 = predict(ml0, newdata = test0)
data.frame(
  RMSE = RMSE(yhat.ml0, test0$months),
  R2 = R2(yhat.ml0, test0$months)
)
```



### MLR: Box Cox Transformation
```{r}
boxcox(ml0,lambda = seq(1.7, 3, 0.1), plotit = TRUE)
```

Optimal transformation is for $\lambda = 2.3$

```{r}
summary(bc.transform(data$months, 2.3))
hist(bc.transform(data$months, 2.3))
```




```{r}
ml0.bc <- lm(bc.transform(months, 2.3) ~. , data = train0)
summary(ml0.bc)
```

check diagnostic plots
```{r}
plot(ml0.bc)
```

normality has improved but nothing else ...

```{r}
yhat.ml0.bc = predict(ml0.bc, newdata = test0 %>% mutate(months = bc.transform(test0$months, 2.3)))
data.frame(
  RMSE = RMSE(yhat.ml0.bc, bc.transform(test0$months, 2.3)),
  R2 = R2(yhat.ml0.bc, bc.transform(test0$months, 2.3))
)
```


### MLR: log(360 - y) Transformation
```{r}
hist(log(360-train0$months))
summary(log(360-train0$months))
```


```{r}
ml0.log360 <- lm(log(360-train0$months) ~. , data = train0)
summary(ml0.log360)
```


check diagnostic plots
```{r}
plot(ml0.log360)
```

Predictions and performance diagnostics
```{r}
yhat.ml0.log360 = predict(ml0.log360, newdata = test0 %>% mutate(months = log(360 - test0$months)))
data.frame(
  RMSE = RMSE(yhat.ml0.log360, log(360 - test0$months)),
  R2 = R2(yhat.ml0.log360, log(360 - test0$months))
)
```



### MLR: sqrt(350 - y) Transformation
```{r}
hist(sqrt(350-train0$months))
summary(sqrt(350-train0$months))
```


```{r}
ml0.sqrt350 <- lm(sqrt(350-train0$months) ~. , data = train0)
summary(ml0.sqrt350)
```


check diagnostic plots
```{r}
plot(ml0.sqrt350)
```

Predictions and performance diagnostics
```{r}
yhat.ml0.sqrt350 = predict(ml0.sqrt350, newdata = test0 %>% dplyr::mutate(months = sqrt(350-test0$months)))
data.frame(
  RMSE = RMSE(yhat.ml0.sqrt350, sqrt(350-test0$months)),
  R2 = R2(yhat.ml0.sqrt350, sqrt(350-test0$months))
)
```

# Applying the log(360 - y) transform to all data
```{r}
train0$months <- log(360 - train0$months)
test0$months <- log(360 - test0$months)

train1$months <- log(360 - train1$months)
test1$months <- log(360 - test1$months)
```



# Multiple linear regression for mode 0 songs 

## Full Multiple Linear Regression 
```{r}
set.seed(123)
mlr0 <- lm(months ~. , data = train0)
```

assess model assumptions
```{r}
plot(mlr0)
```

assessing multicolinearity
```{r}
vif(mlr0)
```

Since none of these GVIF values are above 4, we can conclude there are no issues of multicollinearity.

view model
```{r}
summary(mlr0)
```

All variables except Key1, Key2, Key3, Kdy5, Key6, Key7, and Key8 are have a significant linear relationship with the month of a song's release. With an Adjusted R-squared of 0.4069, 40.69% of variation in months can be explained by the significant independent variables.

The Full Multiple Linear Regression model for mode 0 songs is written below with coefficients rounded to the thousandths.
$$
log(360 - \hat{months}) = 2.739 - 0.022{popularity} + 0.194{duration} - 0.240{acousticness} 
\\ + 0.424{danceability} +  0.572{energy} - 0.848{instrumentalness} 
\\ - 0.013{key1} + 0.016{key2} + 0.035{key3} + 0.118{key4} + 0.017{key5} + 0.051{key6}
\\ - 0.087{key7}  -0.056{key8} +  0.109{key9} + 0.126{key10} + 0.101{key11}
\\ - 0.087{loudness} - 0.330{speechiness} + 0.001{tempo} + 0.324{valence}
$$

```{r}
# prediction on test data
yhat.mlr0 = predict(mlr0, newdata=test0)
# RMSE for test data
error.mlr0 <- yhat.mlr0 - test0$months
rmse.mlr0 <- sqrt(mean(error.mlr0^2))
data.frame(
  RMSE = RMSE(yhat.mlr0, test0$months),
  R2 = R2(yhat.mlr0, test0$months)
)
```


## Variable Selection: Stepwise 10 fold Cross Validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv0 <- train(months ~. , data = train0,  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv0)
summary(mlr.step_kcv0$finalModel)
```

The Stepwise Multiple Linear Regression model for mode 0 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 2.732 - 0.022{popularity} + 0.193{duration} - 0.241{acousticness} 
\\ + 0.425{danceability} +  0.572{energy} - 0.852{instrumentalness} 
\\ + 0.125{key4} + 0.057{key6}
\\ +  0.115{key9} + 0.133{key10} + 0.107{key11}
\\ - 0.087{loudness} - 0.328{speechiness} + 0.001{tempo} + 0.324{valence}
$$

Compared to the full multiple linear regression model, the stepwise regression model omits the variables: Key1, Key2, Key3, Key5, Key7, and Key8. Furthermore, all the variables except Key6 have a significant linear relationship with the response. The percent of variation explained is just .05% larger than that of the full multiple linear regression model at 40.74% of the variance explained.


prediction on test data
```{r}
# prediction on test data
yhat.step_kcv0 = predict(mlr.step_kcv0$finalModel, newdata=key.dummy(test0))
# RMSE for test data
error.step_kcv0 <- yhat.step_kcv0 - test0$months
rmse.step_kcv0 <- sqrt(mean(error.step_kcv0^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv0, test0$months),
  R2 = R2(yhat.step_kcv0, test0$months)
)
```

## Variable Selection: All Subsets Regression (Not using K-folds CV)
```{r}
set.seed(123)
allreg.models0 <- regsubsets(months ~., data = train0, nvmax = 21)
summary(allreg.models0)
```

Assess models
```{r}
allreg.res0 <- summary(allreg.models0)
allreg.compare0 <- data.frame(model = c(1:21),
                              Adj.R2 = allreg.res0$adjr2,
                              CP = allreg.res0$cp)
allreg.compare0
```

The model with the smallest CP value and greatest Adjusted R2 value is model number 15.
```{r}
set.seed(123)
mlr.allreg0 <- lm(months ~ popularity +duration +acousticness +danceability +energy +instrumentalness+
  key4+key6+key9 +key10 +key11 +loudness +speechiness+ tempo +valence, data = key.dummy(train0))
summary(mlr.allreg0)
```

The All Subsets Regression Multiple Linear Regression model for mode 0 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 2.733 - 0.022{popularity} + 0.193{duration} - 0.241{acousticness} 
\\ + 0.425{danceability} +  0.572{energy} - 0.852{instrumentalness} 
\\ + 0.125{key4} + 0.057{key6}
\\ +  0.115{key9} + 0.133{key10} + 0.107{key11}
\\ - 0.087{loudness} - 0.328{speechiness} + 0.001{tempo} + 0.324{valence}
$$

The multiple linear regression model result from the all subsets regression model is the same as that from the stepwise method. It also has the same significant variables and the adjusted R squared value.

prediction on test data
```{r}
# prediction on test data
yhat.allreg0 = predict(mlr.allreg0, newdata=key.dummy(test0))
# RMSE for test data
# error.allreg0 <- yhat.allreg0 - test0$months
# rmse.allreg0 <- sqrt(mean(error.allreg0^2))
data.frame(
  RMSE = RMSE(yhat.allreg0, test0$months),
  R2 = R2(yhat.allreg0, test0$months)
)
```




## Regularized Regression: Ridge
```{r}
set.seed(123)
lm.ridge0 <- train(months ~. , data = train0, method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge0$bestTune
# best coefficient
lm.ridge0.model <- coef(lm.ridge0$finalModel, lm.ridge0$bestTune$lambda)
lm.ridge0.model
```

The Ridge Multiple Linear Regression model for mode 0 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 2.866 - 0.021{popularity} + 0.190{duration} - 0.245{acousticness} 
\\ + 0.413{danceability} +  0.482{energy} - 0.749{instrumentalness} 
\\ - 0.034{key1} - 0.006{key2} + 0.008{key3} + 0.095{key4} - 0.007{key5} + 0.023{key6}
\\ - 0.063{key7} - 0.075{key8} + 0.082{key9} + 0.099{key10} + 0.078{key11}
\\ - 0.080{loudness} - 0.308{speechiness} + 0.001{tempo} + 0.323{valence}
$$

```{r}
# prediction on test data
yhat.ridge0 = predict(lm.ridge0, s=lm.ridge0$bestTune,newdata=test0)
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge0, test0$months),
  R2 = R2(yhat.ridge0, test0$months)
)
```


## Regularized Regression: Lasso
```{r}
set.seed(123)
lm.lasso0 <- train(months ~. , data = train0, method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso0$bestTune
# best coefficient
lm.lasso0.model <- coef(lm.lasso0$finalModel, lm.lasso0$bestTune$lambda)
lm.lasso0.model
```

The Lasso Multiple Linear Regression model for mode 0 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 2.837 - 0.022{popularity} + 0.191{duration} - 0.240{acousticness} 
\\ + 0.409{danceability} + 0.533{energy} - 0.808{instrumentalness} 
\\ - 0.027{key1} + 0.004{key3} + 0.092{key4} + 0.024{key6}
\\ - 0.056{key7} - 0.068{key8} + 0.082{key9} + 0.099{key10} + 0.077{key11}
\\ - 0.084{loudness} - 0.280{speechiness} + 0.001{tempo} + 0.321{valence}
$$

Compared to the full model, the variables key2 and key 5 are not kept in the model.


```{r}
# prediction on test data
yhat.lasso0 = predict(lm.lasso0, s=lm.lasso0$bestTune,newdata=test0)
# RMSE for test data
error.lasso0 <- yhat.lasso0 - test0$months
rmse.lasso0 <- sqrt(mean(error.lasso0^2))
data.frame(
  RMSE = RMSE(yhat.lasso0, test0$months),
  R2 = R2(yhat.lasso0, test0$months)
)
```


## Deciding on the best model

```{r}
results0 <- data.frame(Model = c("FullMLR", "Stepwise", "AllSubsets","Ridge", "Lasso"),
                       RMSE = c(0.5976752 , 0.5980156, 0.5980156,0.5981751,0.5975859),
                       R2 = c(0.3932124, 0.3925419, 0.3925419,0.3921513,0.3932411))
results0
```

With the lowest RMSE and the Highest R2 value resulting from the prediction assessments, Lasso Provides the best model for predicting the month a song was released based on the Spotify audio features.



$$
log(360 - \hat{months}) = 2.837 - 0.022{popularity} + 0.191{duration} - 0.240{acousticness} 
\\ + 0.409{danceability} + 0.533{energy} - 0.808{instrumentalness} 
\\ - 0.027{key1} + 0.004{key4} + 0.092{key4} + 0.024{key6}
\\ - 0.056{key7} - 0.068{key8} + 0.082{key9} + 0.099{key10} + 0.077{key11}
\\ - 0.084{loudness} - 0.280{speechiness} + 0.001{tempo} + 0.321{valence}
$$

The Lasso model back to original scale would be:
$$
\hat{months} = 360 - \frac{exp(2.837 + 0.191{duration} + 0.409{danceability} + 0.533{energy}+ 0.004{key4} + 0.092{key4} + 0.024{key6}+ 0.082{key9} + 0.099{key10} + 0.077{key11} + 0.001{tempo} + 0.321{valence})}{exp(0.022{popularity} + 0.240{acousticness} + 0.808{instrumentalness} + 0.027{key1} + 0.056{key7} + 0.068{key8}+ 0.084{loudness} + 0.280{speechiness})}
$$

# Multiple linear regression for mode 1 songs 

## Full Multiple Linear Regression 
```{r}
set.seed(123)
mlr1 <- lm(months ~. , data = train1)
```

assess model assumptions
```{r}
plot(mlr1)
```

view model
```{r}
summary(mlr1)
```

The Full Multiple Linear Regression model for mode 1 songs is written below with coefficients rounded to the thousandths.
$$
log(360 - \hat{months}) = 2.990 - 0.020{popularity} + 0.125{duration} - 0.127{acousticness} 
\\ + 0.631{danceability} +  0.383{energy} - 0.771{instrumentalness} 
\\ + 0.084{key1} + 0.029{key2} + 0.020{key3} + 0.068{key4} + 0.075{key5} + 0.096{key6}
\\ + 0.102{key7} + 0.119{key8} + 0.012{key9} + 0.081{key10} + 0.046{key11}
\\ - 0.076{loudness} - 0.180{speechiness} + 0.001{tempo} + 0.294{valence}
$$


The summary of the model show us that the variables with a significant linear relationship to the response of month the song was released are: Popularity, Duration, Acousticness, Danceability, Energy, Instrumentalness, Key1, Key6, Key7, Key8, Loudness, Tempo and Valence.



```{r}
# prediction on test data
yhat.mlr1 = predict(mlr1, newdata=test1)
# RMSE for test data
error.mlr1 <- yhat.mlr1 - test1$months
rmse.mlr1 <- sqrt(mean(error.mlr1^2))
data.frame(
  RMSE = RMSE(yhat.mlr1, test1$months),
  R2 = R2(yhat.mlr1, test1$months)
)
```

## Variable Selection: Stepwise 10 fold Cross Validation
```{r}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 
bye <- capture.output(mlr.step_kcv1 <- train(months ~. , data = train1,  
                 method = "lmStepAIC", trControl = train_control)) 
print(mlr.step_kcv1)
summary(mlr.step_kcv1$finalModel)
```

The Multiple Linear Regression model resulting from stepwise 10 fold cross validation for mode 1 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 3.020 - 0.020{popularity} + 0.127{duration} - 0.124{acousticness} 
\\ + 0.630{danceability} +  0.357{energy} - 0.765{instrumentalness} + 0.062{key1} 
\\ + 0.053{key5} + 0.073{key6} + 0.079{key7} + 0.096{key8} + 0.060{key10} 
\\ - 0.074{loudness}   + 0.001{tempo} + 0.289{valence}
$$

Compared to the full multiple linear regression model, the stewise multiple regression leaves out the dummy variables of Key2, key3, key4, key11 and speechiness. All of the variables in the model besides Key5 and Key10 have a significant linear relationship with month of song release.

prediction on test data
```{r}
# prediction on test data
yhat.step_kcv1 = predict(mlr.step_kcv1$finalModel, newdata=key.dummy(test1))
# RMSE for test data
error.step_kcv1 <- yhat.step_kcv1 - test1$months
rmse.step_kcv1 <- sqrt(mean(error.step_kcv1^2))
data.frame(
  RMSE = RMSE(yhat.step_kcv1, test1$months),
  R2 = R2(yhat.step_kcv1, test1$months)
)
```

## Variable Selection: All Subsets Regression (Not using K-folds CV)
```{r}
set.seed(123)
allreg.models1 <- regsubsets(months ~., data = train1, nvmax = 21)
summary(allreg.models1)
```


```{r}
allreg.res1 <- summary(allreg.models1)
allreg.compare1 <- data.frame(model = c(1:21),
                              Adj.R2 = allreg.res1$adjr2,
                              CP = allreg.res1$cp)
allreg.compare1
```


The model with the smallest CP value and greatest Adjusted R2 value is model number 15.
```{r}
set.seed(123)
mlr.allreg1 <- lm(months ~ popularity +duration +acousticness +danceability +energy +instrumentalness+
  key1+key5 +key6 +key7 +key8+loudness + tempo +valence, data = key.dummy(train1))
summary(mlr.allreg1)
```


The Multiple Linear Regression model resulting from all subsets regression for mode 1 songs is written below with coefficients rounded to the thousandths.

$$
log(360 - \hat{months}) = 3.033 - 0.020{popularity} + 0.128{duration} - 0.122{acousticness} 
\\ + 0.630{danceability} +  0.357{energy} - 0.768{instrumentalness} + 0.057{key1} 
\\ + 0.048{key5} + 0.068{key6} + 0.074{key7} + 0.091{key8}
\\ - 0.074{loudness}   + 0.001{tempo} + 0.289{valence}
$$

Compared to the full multiple linear regression model, the all subsets multiple regression leaves out the dummy variables of Key2, key3, key4, Key10, key11 and Speechiness. All of the variables in the model besides Key5 have a significant linear relationship with month of song release. Furthermore, the model is very similar to that resulting from the stepwise method, however, this model has removed key 10 which was insignificant in the wtepwise model.

prediction on test data
```{r}
# prediction on test data
yhat.allreg1 = predict(mlr.allreg1, newdata=key.dummy(test1))
# RMSE for test data
# error.allreg0 <- yhat.allreg0 - test0$months
# rmse.allreg0 <- sqrt(mean(error.allreg0^2))
data.frame(
  RMSE = RMSE(yhat.allreg1, test1$months),
  R2 = R2(yhat.allreg1, test1$months)
)
```

## Regularized Regression: Ridge
```{r}
set.seed(123)
lm.ridge1 <- train(months ~. , data = train1, method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.ridge1$bestTune
# best coefficient
lm.ridge1.model <- coef(lm.ridge1$finalModel, lm.ridge1$bestTune$lambda)
lm.ridge1.model
```


The Ridge Multiple Linear Regression model for mode 1 songs is written below with coefficients rounded to the thousandths.
$$
log(360 - \hat{months}) = 3.143 - 0.019{popularity} + 0.122{duration} - 0.142{acousticness} 
\\ + 0.597{danceability} +  0.279{energy} - 0.690{instrumentalness} 
\\ + 0.066{key1} + 0.012{key2} + 0.001{key3} + 0.050{key4} + 0.056{key5} + 0.077{key6}
\\ + 0.081{key7} + 0.096{key8} - 0.005{key9} + 0.065{key10} + 0.028{key11}
\\ - 0.068{loudness} - 0.158{speechiness} + 0.001{tempo} + 0.297{valence}
$$

The ridge model keeps all variables, therefore, there have been none removed for the model. However, the ridge model does aim to reduce all coefficients closer to zero. 

```{r}
# prediction on test data
yhat.ridge1 = predict(lm.ridge1, s=lm.ridge1$bestTune,newdata=test1)
# RMSE for test data
data.frame(
  RMSE = RMSE(yhat.ridge1, test1$months),
  R2 = R2(yhat.ridge1, test1$months)
)
```


## Regularized Regression: Lasso
```{r}
set.seed(123)
lm.lasso1 <- train(months ~. , data = train1, method = "glmnet",
                      trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
#Best Tuning Parameter
lm.lasso1$bestTune
# best coefficient
lm.lasso1.model <- coef(lm.lasso1$finalModel, lm.lasso1$bestTune$lambda)
lm.lasso1.model
```

The Lasso Multiple Linear Regression model for mode 1 songs is written below with coefficients rounded to the thousandths.
$$
log(360 - \hat{months}) = 3.055 - 0.020{popularity} + 0.123{duration} - 0.127{acousticness} 
\\ + 0.622{danceability} +  0.354{energy} - 0.750{instrumentalness} 
\\ + 0.066{key1} + 0.010{key2} + 0.047{key4} + 0.055{key5} + 0.077{key6}
\\ + 0.084{key7} + 0.0997{key8} + 0.061{key10} + 0.027{key11}
\\ - 0.073{loudness} - 0.148{speechiness} + 0.001{tempo} + 0.291{valence}
$$

Compared to the full model, the Lasso model omits dummy variables Key3 and Key9

```{r}
# prediction on test data
yhat.lasso1 = predict(lm.lasso1, s=lm.lasso1$bestTune,newdata=test1)
# RMSE for test data
error.lasso1 <- yhat.lasso1 - test1$months
rmse.lasso1 <- sqrt(mean(error.lasso1^2))
data.frame(
  RMSE = RMSE(yhat.lasso1, test1$months),
  R2 = R2(yhat.lasso1, test1$months)
)
```


## Determining Best Model and Interpreting.

We will be using the measure of Root Mean Square Error (RMSE) and the R-Squared value to assess the performance of these models. RMSE is calculated: 
$$
\sqrt {\frac{1}{n} \Sigma^{n}_{i=1}(y_i - \hat {y_i})^2}
$$
where $y_i$ are the observed values for month of release since 1992, $\hat {y_i}$ are the predicted month values, and $i = 1, ... , n$ where $n = 1835$ for the test data of songs that are mode 1. 

```{r}
results1 <- data.frame(Model = c("FullMLR", "Stepwise", "AllSubsets","Ridge", "Lasso"),
                       RMSE = c(0.6078257, 0.6070693,0.6071187,0.607864,0.6073883),
                       R2 = c(0.3312429, 0.3329448, 0.3328339,0.332307,0.3322821))
results1
```

The model with the smallest RMSE (on the log(360 - y) scale) and largest R-Squared values is the Multiple Linear Regression from the Stepwise method using 10 fold cross validation.

Stepwise regression is the best model.
```{r}
summary(mlr.step_kcv1$finalModel)
```

$$
log(360 - \hat{months}) = 3.020 - 0.020{popularity} + 0.127{duration} - 0.124{acousticness} 
\\ + 0.630{danceability} +  0.357{energy} - 0.765{instrumentalness} + 0.062{key1} 
\\ + 0.053{key5} + 0.073{key6} + 0.079{key7} + 0.096{key8} + 0.060{key10} 
\\ - 0.074{loudness}   + 0.001{tempo} + 0.289{valence}
$$

Converting it back to normal scale would result in the following 
$$
\hat{months} = 360  - \frac{exp(3 + 0.127{duration} + 0.630{danceability} +  0.357{energy} + 0.062{key1} + 0.053{key5} + 0.073{key6} + 0.079{key7} + 0.096{key8} + 0.060{key10} + 0.001{tempo} + 0.289{valence})}{exp(0.020{popularity} + 0.124{acousticness} - 0.765{instrumentalness} - 0.074{loudness} )}
$$


