---
title: "Classification (Supervised Learning): Logistic Regression"
author: "Mary Solomon"
date: "2/9/2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales) #for formatting with percentage signs
library(nnet) #for multinomial regression
library(caret)
library(leaps) # for all possible regressions approach LINEAR
library(glmulti)# for all possible regressions approach LOGISTIC
```

load the functions
```{r}
source("thesis_functions.R")
```


set levels of ordinal data, and other variables to their appropriate dataypes. Did not adjust the datatypes for date/time since those will not be used in this portion
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               #release_date = as.POSIXct.Date(release_date),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)))
data$newgen <- as.factor(ifelse(data$ArtistGen == 3 | data$ArtistGen == 4, 1, 0))
```


Variables being kept for classification and cluster analysis: popularity, duration, acousticness, danceability, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, valence. These are the predictor variables that will be used to classify on outcome of ArtistGeneration, or to determine the optimal clusters in the case of a clustering problem. 
Any information about date of song release or date of artist promotions are excluded since these are directly correlated to kpop generations. rather, we just want to make predictions on whether we can classify the kpop songs into their kpop generations based on qualities of the music such as the audio features.

Time signature is out since almost all are 4/4 time. Analysis will also be split up into two binary logistic regression models which will predict for songs where mode = 0 and those where mode = 1

Personal predictions: I believe popularity will be a strong predictor since the popularity measure is also time dependent.

# Assumptions of the logistic Model


# Logistic: predict which songs are old gen (1 & 2) vs new gen (3 & 4)

Create Train(70%), Valid(15%), and Test(15%) Data
```{r}
kpop <- select(data, newgen, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% filter(mode == 0)%>% select(-mode)
kpop1 <- kpop %>% filter(mode == 1) %>% select(-mode)

# Mode0: Train/Valid/Test
set.seed(123)
p3.0 <- partition.3(kpop0, 0.70, 0.15)
train0 <- p3.0$data.train
valid0 <- p3.0$data.val
test0 <- p3.0$data.test
all.train0 <- rbind(train0, valid0)

# Mode1: Train/Valid/Test
set.seed(123)
p3.1 <- partition.3(kpop1, 0.70, 0.15)
train1 <- p3.1$data.train
valid1 <- p3.1$data.val
test1 <- p3.1$data.test
all.train1 <- rbind(train1, valid1)
```



## Logistic Where mode = 0
Minor key

Full logistic model
```{r}
set.seed(123)
logit0 <- glm(newgen ~., data = train0, family = "binomial")
summary(logit0)
```

find optimal cut off value
```{r, warning = FALSE}
logit.out0 <- opt.cut.genn(logit0, valid0)
opt.cut.plot(logit.out0)
#optimal by kappa score
logit.out0$cutoff[which.max(logit.out0$kappa.vec)]
#optimal by sensitivity/specificity balance
logit.out0$cutoff[which.min(logit.out0$ssdiff.vec)]
```

Final full logistic model fit to the combo of train and validation data.
```{r}
set.seed(123)
logit0.final <- glm(newgen ~., data = all.train0, family = "binomial")
summary(logit0.final)
```

Full Logistic model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.453 + 0.074x_{Popularity} - 0.009x_{Duration} + 1.280x_{Acousticness} + 0.715x_{Danceability} 
\\ + 0.227x_{Energy} + 2.790x_{Instrumentalness} + 0.301x_{Key1} + 0.242x_{Key2} 
\\ - 0.540x_{Key3} - 0.308x_{Key4} - 0.253x_{Key5} - 0.148x_{Key6} + 0.498x_{Key7} 
\\ - 0.194x_{Key8} - 0.270x_{Key9} - 0.307x_{Key10} - 0.075x_{Key11} + 0.236x_{Loudness}
\\ + 2.170x_{Speechiness} - 0.003x_{Tempo} - 1.359x_{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Energy, Instrumentalness, Loudness, Speechiness and choosing Key1(C#/D-flat minor), Key2(D minor), Key7(G minor) instead of Key0 (C minor). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Tempo, Valence and choosing Key3(D#/E-flat Minor), Key4(E minor), Key5(F Minor), Key6(F#/G-flat Minor), Key8 (G#/A-flat minor), Key9(A minor), Key10(A#/B-flat minor), Key11(B minor) instead of Key0(C).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Key7(G minor), Loudness, Speechiness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation. 



confusion matrix on the training data:
```
logit.train0.pred <- logit0$fitted.values
logit.train0.y <- ifelse(logit.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.train0.y), as.factor(train0$newgen), 
                positive = "1")
```


confusion matrix on the test data using standard cut off 
```{r}
#get predicted probabilities for the test data
logit.test0.pred <- predict(logit0.final, newdata = test0, type = "response")
logit.test0.y <- ifelse(logit.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

confusion matrix on the test data using optimal cutoff based on kappa (0.40)
```{r}
#get predicted probabilities for the test data
logit.test0.pred <- predict(logit0.final, newdata = test0, type = "response")
logit.test0.y <- ifelse(logit.test0.pred > 0.40, 1, 0) # using cutoff = 0.40
confusionMatrix(as.factor(logit.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

confusion matrix on the test data using optimal cutoff based on sensitivity/specificty balance (0.35)
```{r}
#get predicted probabilities for the test data
logit.test0.pred <- predict(logit0.final, newdata = test0, type = "response")
logit.test0.y <- ifelse(logit.test0.pred > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(logit.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

### Variable Selection: Stepwise
stepwise 
```{r}
set.seed(123)
logit0.none <- glm(newgen ~ 1, data = train0, family = binomial)
step0 <- step(logit0.none, 
                 list(lower=formula(logit0.none), upper = formula(logit0)),
                 direction = "both",
                 trace=0)
formula(step0)
summary(step0)
```

find optimal cut off value
```{r, warning = FALSE}
step.out0 <- opt.cut.genn(step0, valid0)
opt.cut.plot(step.out0)
#optimal by kappa score
step.out0$cutoff[which.max(step.out0$kappa.vec)]
#optimal by sensitivity/specificity balance
step.out0$cutoff[which.min(step.out0$ssdiff.vec)]
```

final stepwise model
```{r}
set.seed(123)
logit0.none <- glm(newgen ~ 1, data = all.train0, family = binomial)
step0.final <- step(logit0.none, 
                 list(lower=formula(logit0.none), upper = formula(logit0)),
                 direction = "both",
                 trace=0)
formula(step0.final)
summary(step0.final)
```

Stepwise Logistic Model:    

In comparison to the full model, the variable energy is not included. Otherwise, many of the coefficients are very similar to that of the full model. 

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.684 + 0.074_{Popularity} - 0.009_{Duration} + 1.221_{Acousticness} 
\\ + 0.695_{Danceability} + 2.807_{Instrumentalness} + 0.298_{Key1} + 0.240_{Key2} 
\\ - 0.545_{Key3} - 0.308x_{Key4} - 0.256x_{Key5} - 0.153_{Key6} + 0.497_{Key7} 
\\ - 0.197_{Key8} - 0.271_{Key9} - 0.310_{Key10} - 0.076_{Key11} + 0.246_{Loudness}
\\ + 2.214_{Speechiness} - 0.003_{Tempo} - 1.331_{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness and choosing Key1(C#/D-flat minor), Key2(D minor), Key7(G minor) instead of Key0 (C minor). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Tempo, Valence and choosing Key3(D#/E-flat Minor), Key4(E minor), Key5(F Minor), Key6(F#/G-flat Minor), Key8 (G#/A-flat minor), Key9(A minor), Key10(A#/B-flat minor), Key11(B minor) instead of Key0(C).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Key7(G minor), Loudness, Speechiness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation. These significant variables are the same as the full model.

confusion matrix on the training data:
```
step.train0.pred <- step0$fitted.values
step.train0.y <- ifelse(step.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.train0.y), as.factor(train0$newgen), 
                positive = "1")
```


predict and evaluate on test using standard cut off 0.5
```{r}
#get predicted probabilities for the test data
step.test0.pred <- predict(step0.final, newdata = test0, type = "response")
step.test0.y <- ifelse(step.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on test using optimal cut off 0.46 for highest kappa
```{r}
#get predicted probabilities for the test data
step.test0.pred <- predict(step0.final, newdata = test0, type = "response")
step.test0.y <- ifelse(step.test0.pred > 0.46, 1, 0) # using cutoff = 0.46
confusionMatrix(as.factor(step.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on test using optimal cut off 0.36 for best sensitivity/specificity balance
```{r}
#get predicted probabilities for the test data
step.test0.pred <- predict(step0.final, newdata = test0, type = "response")
step.test0.y <- ifelse(step.test0.pred > 0.36, 1, 0) # using cutoff = 0.36
confusionMatrix(as.factor(step.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
bye <- capture.output(step0_kcv <- train(newgen ~ ., data = train0, family = "binomial", 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE)) 
```

```{r}
step0_kcv <- step0_kcv$finalModel
summary(step0_kcv)
```


find optimal cut off value
```{r, warning = FALSE}
step_kcv.out0 <- opt.cut.genn(step0_kcv, key.dummy(valid0))
opt.cut.plot(step_kcv.out0)
#optimal by kappa score
step_kcv.out0$cutoff[which.max(step_kcv.out0$kappa.vec)]
#optimal by sensitivity/specificity balance
step_kcv.out0$cutoff[which.min(step_kcv.out0$ssdiff.vec)]
```

Fit final model on combination of training and validation data
```{r}
set.seed(123)
# Fit K-fold CV model  
bye2 <- capture.output(step0_kcv.final <- train(newgen ~ ., data = all.train0, family = "binomial", 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE)) 
```

```{r}
step0_kcv.final <- step0_kcv.final$finalModel
summary(step0_kcv.final)
```


Stepwise 10kcv Logistic Model:

Compared to The full model, energy, key3 - key6, and key9 - key11 are not included in this model. 

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.480 + 0.074{Popularity} - 0.009{Duration} + 1.216{Acousticness} 
\\ + 0.672{Danceability} + 2.807{Instrumentalness} + 0.503{Key1} + 0.445{Key2}
\\ + 0.702{Key7} + 0.245{Loudness} + 2.207{Speechiness} - 0.003{Tempo} - 1.319{Valence}
$$
The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness and choosing Key1(C#/D-flat minor), Key2(D minor), Key7(G minor) instead of Key0 (C minor). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Tempo, and Valence.

Almost all the variables in the model: Popularity, Duration, Acousticness, Instrumentalness, Key1(C#/D-flat minor), Key2(D minor), Key7(G minor), Loudness, Speechiness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation. Only the predictors Tempo and Danceability are not significant. 

```
#get predicted probabilities for training data
step_kcv.train0.pred <- step0_kcv$fitted.values
step_kcv.train0.y <- ifelse(step_kcv.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.train0.y), as.factor(train0$newgen), 
                positive = "1")
```

Get prediction and performance evaluation for standard cut off 0.5
```{r}
#get predicted probabilities for the test data
step_kcv.test0.pred <- predict(step0_kcv.final, newdata = key.dummy(test0), type = "response")
step_kcv.test0.y <- ifelse(step_kcv.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

Get prediction and performance evaluation for optimal cut off 0.45 for highest kappa
```{r}
#get predicted probabilities for the test data
step_kcv.test0.pred <- predict(step0_kcv.final, newdata = key.dummy(test0), type = "response")
step_kcv.test0.y <- ifelse(step_kcv.test0.pred > 0.45, 1, 0) # using cutoff = 0.45
confusionMatrix(as.factor(step_kcv.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

Get prediction and performance evaluation for optimal cut off 0.35 for best sensitivity/specificity balance
```{r}
#get predicted probabilities for the test data
step_kcv.test0.pred <- predict(step0_kcv.final, newdata = key.dummy(test0), type = "response")
step_kcv.test0.y <- ifelse(step_kcv.test0.pred > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(step_kcv.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

### Variable Selection: All possible regression

```{r}
set.seed(123)
glmulti.out0 <- glmulti(newgen ~ ., data = train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out0@formulas
```

view summary of top model
```{r}
summary(glmulti.out0@objects[[1]])
```



Store model
```{r}
allreg.logit0 <- glmulti.out0@objects[[1]]
```


find optimal cut off value
```{r, warning = FALSE}
allreg.out0 <- opt.cut.genn(allreg.logit0, valid0)
opt.cut.plot(allreg.out0)
#optimal by kappa score
allreg.out0$cutoff[which.max(allreg.out0$kappa.vec)]
#optimal by sensitivity/specificity balance
allreg.out0$cutoff[which.min(allreg.out0$ssdiff.vec)]
```

Fit final model on combo of train/validation data
```{r}
set.seed(123)
glmulti.out0.final <- glmulti(newgen ~ ., data = all.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out0.final@formulas
summary(glmulti.out0.final@objects[[1]])
allreg.logit0.final <- glmulti.out0.final@objects[[1]]
```


All Possible Regressions Model

Compared to the full model, Energy is not included in this model. Besides this difference, the full logit model and this one resulting from the all subsets method are very similar.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.684 + 0.074x_{Popularity} - 0.009x_{Duration} + 1.221x_{Acousticness} 
\\ + 0.695_{Danceability} + 2.807x_{Instrumentalness} + 0.298x_{Key1} + 0.240x_{Key2} 
\\ - 0.545x_{Key3} - 0.308x_{Key4} - 0.256x_{Key5} - 0.153x_{Key6} + 0.497x_{Key7} 
\\ - 0.197x_{Key8} - 0.271x_{Key9} - 0.310x_{Key10} - 0.076x_{Key11} + 0.246x_{Loudness}
\\ + 2.214x_{Speechiness} - 0.003x_{Tempo} - 1.331x_{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness and choosing Key1(C#/D-flat minor), Key2(D minor), Key7(G minor) instead of Key0 (C minor). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Tempo, Valence and choosing Key3(D#/E-flat Minor), Key4(E minor), Key5(F Minor), Key6(F#/G-flat Minor), Key8 (G#/A-flat minor), Key9(A minor), Key10(A#/B-flat minor), Key11(B minor) instead of Key0(C).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Key7(G minor), Loudness, Speechiness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation. 


try fitting training data to results from all possible regression variable selection
```
#get predicted probabilities for training data
all.train0.pred <- allreg.logit0$fitted.values
all.train0.y <- ifelse(all.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.train0.y), as.factor(train0$newgen), 
                positive = "1")
```

```{r}
#get predicted probabilities for the test data
all.test0.pred <- predict(allreg.logit0.final, newdata = test0, type = "response")
all.test0.y <- ifelse(all.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.test0.y), as.factor(test0$newgen), 
                positive = "1")
```

Using the cutoff that yields the optimal kappa (0.46)
```{r}
#get predicted probabilities for the test data
all.test0.pred <- predict(allreg.logit0.final, newdata = test0, type = "response")
all.test0.y <- ifelse(all.test0.pred > 0.46, 1, 0) # using cutoff = 0.46
confusionMatrix(as.factor(all.test0.y), as.factor(test0$newgen), 
                positive = "1")
```


Using the cutoff that yields the best balance between sensitivity/specificity (0.36)
```{r}
#get predicted probabilities for the test data
all.test0.pred <- predict(allreg.logit0.final, newdata = test0, type = "response")
all.test0.y <- ifelse(all.test0.pred > 0.36, 1, 0) # using cutoff = 0.36
confusionMatrix(as.factor(all.test0.y), as.factor(test0$newgen), 
                positive = "1")
```



### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
set.seed(123)
ridge0 <- train(newgen ~ ., data = train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0$bestTune
ridge0.model <- coef(ridge0$finalModel, ridge0$bestTune$lambda)
ridge0.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
ridge0.out <- reg.opt.cut.genn(ridge0, valid0)
opt.cut.plot(ridge0.out)
# cut off by kappa
ridge0.out$cutoff[which.max(ridge0.out$kappa.vec)]
ridge0.out$cutoff[which.min(ridge0.out$ssdiff.vec)]
```

create final model
```{r}
set.seed(123)
ridge0.final <- train(newgen ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge0.final$bestTune
ridge0.model.final <- coef(ridge0.final$finalModel, ridge0.final$bestTune$lambda)
ridge0.model.final
```

Ridge 10 fold Cross Validation model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.457 + 0.060{Popularity} - 0.008{Duration} + 0.904{Acousticness} + 0.281{Danceability} 
\\ + 0.160{Energy} + 1.840{Instrumentalness} + 0.288{Key1} + 0.236{Key2} 
\\ - 0.358{Key3} - 0.202{Key4} - 0.137{Key5} - 0.048{Key6} + 0.445{Key7} 
\\ - 0.120{Key8} - 0.177{Key9} - 0.191{Key10} - 0.030{Key11} + 0.175{Loudness}
\\ + 1.769{Speechiness} - 0.002{Tempo} - 1.020{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge0 <- predict(ridge0.final, s = ridge0.final$bestTune, test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.44 corresponding to optimal kappa
```{r}
prob.ridge0 <- predict(ridge0.final, s = ridge0.final$bestTune, test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.44, 1, 0) # using cutoff = 0.44
confusionMatrix(as.factor(pred.ridge0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.37 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.ridge0 <- predict(ridge0.final, s = ridge0.final$bestTune, test0, type = "prob")
pred.ridge0 <- ifelse(prob.ridge0[,2] > 0.37, 1, 0) # using cutoff = 0.37
confusionMatrix(as.factor(pred.ridge0), as.factor(test0$newgen), 
                positive = "1")
```

### Regularized Regression: Lasso 10 fold Cross Validation
```{r}
set.seed(123)
lasso0 <- train(newgen ~ ., data = train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso0$bestTune
# best coefficient
lasso0.model <- coef(lasso0$finalModel, lasso0$bestTune$lambda)
lasso0.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
lasso0.out <- reg.opt.cut.genn(lasso0, valid0)
opt.cut.plot(lasso0.out)
# cut off by kappa
lasso0.out$cutoff[which.max(lasso0.out$kappa.vec)]
lasso0.out$cutoff[which.min(lasso0.out$ssdiff.vec)]
```

create final model
```{r}
set.seed(123)
lasso0.final <- train(newgen ~ ., data = train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso0.final$bestTune
# best coefficient
lasso0.model.final <- coef(lasso0.final$finalModel, lasso0.final$bestTune$lambda)
lasso0.model.final
```


Lasso 10 fold Cross Validation model:

Compared to the full model, this one leaves out the variables of danceability, energy, Key4 - Key6, Key 8, and Key 11

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.149 + 0.070{Popularity} - 0.008{Duration} + 0.809{Acousticness} + 1.880{Instrumentalness}
\\  
+ 0.204{Key1} + 0.103{Key2} - 0.139{Key3} + 0.526{Key7} - 0.008{Key9} - 0.0004{Key10}
\\  
+ 0.170{Loudness} + 0.947{Speechiness} - 0.001{Tempo} - 0.834{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.lasso0 <- predict(lasso0.final, s = lasso0.final$bestTune, test0, type = "prob")
pred.lasso0 <- ifelse(prob.lasso0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.lasso0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.47 corresponding to optimal kappa
```{r}
prob.lasso0 <- predict(lasso0.final, s = lasso0.final$bestTune, test0, type = "prob")
pred.lasso0 <- ifelse(prob.lasso0[,2] > 0.47, 1, 0) # using cutoff = 0.47
confusionMatrix(as.factor(pred.lasso0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.35 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.lasso0 <- predict(lasso0.final, s = lasso0.final$bestTune, test0, type = "prob")
pred.lasso0 <- ifelse(prob.lasso0[,2] > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(pred.lasso0), as.factor(test0$newgen), 
                positive = "1")
```


### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
set.seed(123)
enet0 <- train(newgen ~ ., data = train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0$bestTune
# best coefficient
enet0.model <- coef(enet0$finalModel, enet0$bestTune$lambda)
enet0.model
```

search for best cutoff with validation set
```{r, warning = FALSE}
enet0.out <- reg.opt.cut.genn(enet0, valid0)
opt.cut.plot(enet0.out)
# cut off by kappa
enet0.out$cutoff[which.max(enet0.out$kappa.vec)]
enet0.out$cutoff[which.min(enet0.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
set.seed(123)
enet0.final <- train(newgen ~ ., data = all.train0, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet0.final$bestTune
# best coefficient
enet0.model.final <- coef(enet0.final$finalModel, enet0.final$bestTune$lambda)
enet0.model.final
```


Elastic Net 10 fold Cross Validation model:

Compared to the full model, this one leaves out the variables of danceability, energy, Key4 - Key6, Key8, Key9, and Key11

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.116 + 0.070{Popularity} - 0.008{Duration} + 0.737{Acousticness} + 1.771{Instrumentalness}
\\  
+ 0.305{Key1} + 0.174{Key2} - 0.129{Key3} + 0.454{Key7} - 0.0004{Key10}
\\  
+ 0.170{Loudness} + 1.154{Speechiness} - 0.001{Tempo} -0.847{Valence}
$$

predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet0 <- predict(enet0.final, s = enet0.final$bestTune, test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.47 corresponding to optimal kappa
```{r}
prob.enet0 <- predict(enet0.final, s = enet0.final$bestTune, test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.47, 1, 0) # using cutoff = 0.47
confusionMatrix(as.factor(pred.enet0), as.factor(test0$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.35 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet0 <- predict(enet0.final, s = enet0.final$bestTune, test0, type = "prob")
pred.enet0 <- ifelse(prob.enet0[,2] > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(pred.enet0), as.factor(test0$newgen), 
                positive = "1")
```


### Deciding on the best model for Mode 0 songs


Performance metrics
```{r}
mode0.results <- data.frame(Model = c("Full", "Step", "Step 10CV", "AllReg", "Ridge 10CV", "Lasso 10CV", "ElasticNet 10CV"),
                            cutoff = c(0.35, 0.36, 0.35, 0.36, 0.37, 0.35, 0.35), #optimal balance between sensitivity and specificity
                            #AIC = c(3815.2, 3813.4, 3804.3, 3813.4, NA, NA, NA),
                            Accuracy = c(0.7175, 0.7247, 0.7218, 0.7247, 0.7261, 0.7361, 0.7375),
                            Kappa = c(0.4222, 0.4342, 0.4320, 0.4342, 0.4360, 0.4597, 0.4616),
                            Sensitivity = c(0.7143, 0.7106, 0.7253, 0.7106, 0.7070, 0.7363, 0.7326),
                            Specificity = c(0.7196, 0.7336, 0.7196, 0.7336, 0.7383, 0.7360, 0.7407))
mode0.results
```

All models have VERY similar results. Therefore, one could argue that each would work as equally as well as one another. Based on the analysis done with this sample, the best model would be the Elastic Net model, as it has the highest accuracy score, as well as the sensitivity and specifity scores simultaneously having fairly high rates. 

Elastic Net 10 fold Cross Validation model:

Compared to the full model, this one leaves out the variables of danceability, energy, Key4 - Key6, Key8, Key9, and Key11

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.116 + 0.070{Popularity} - 0.008{Duration} + 0.737{Acousticness} + 1.771{Instrumentalness}
\\  
+ 0.305{Key1} + 0.174{Key2} - 0.129{Key3} + 0.454{Key7} - 0.0004{Key10}
\\  
+ 0.170{Loudness} + 1.154{Speechiness} - 0.001{Tempo} -0.847{Valence}
$$



## Logistic Where mode = 1

Major key

Full logistic regression model
```{r}
set.seed(123)
logit1 <- glm(newgen ~., data = train1, family = "binomial")
summary(logit1)
```


confusion matrix on the training data:
```
logit.train1.pred <- logit1$fitted.values
logit.train1.y <- ifelse(logit.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.train1.y), as.factor(train1$newgen), 
                positive = "1")
```
find optimal cut off value
```{r, warning = FALSE}
logit.out1 <- opt.cut.genn(logit1, valid1)
opt.cut.plot(logit.out1)
#optimal by kappa score
logit.out1$cutoff[which.max(logit.out1$kappa.vec)]
#optimal by sensitivity/specificity balance
logit.out1$cutoff[which.min(logit.out1$ssdiff.vec)]
```

Final full logistic model fit to the combo of train and validation data.
```{r}
set.seed(123)
logit1.final <- glm(newgen ~., data = all.train1, family = "binomial")
summary(logit1.final)
```

Full Logistic model 

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.783 + 0.079{Popularity} - 0.009{Duration} + 0.882{Acousticness} + 0.665{Danceability} 
\\ + 0.537{Energy} + 2.471{Instrumentalness} - 0.415{Key1} + 0.002{Key2} 
\\ + 0.149{Key3} + 0.280{Key4} - 0.217{Key5} - 0.280{Key6} - 0.161{Key7} 
\\ - 0.185{Key8} + 0.006{Key9} - 0.071{Key10} - 0.094{Key11} + 0.154{Loudness}
\\ + 0.792{Speechiness} + 0.0002{Tempo} - 0.742{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Energy, Instrumentalness, Loudness, Speechiness, Tempo and choosing Key2(D major), Key3(D#/E-flat Major), Key4(E Major), Key9(A Major), instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key5(F Major), Key6(F#/G-flat Major), Key8 (G#/A-flat Major), Key10(A#/B-flat Major), Key11(B Major) instead of Key0(C Major).

Only the variables Popularity, Duration, Acousticness, Danceability, Instrumentalness, Key1(C#/D-flat major), Loudness, Speechiness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation for songs composed in the major mode. 


confusion matrix on the test data using standard cut off 
```{r}
#get predicted probabilities for the test data
logit.test1.pred <- predict(logit1.final, newdata = test1, type = "response")
logit.test1.y <- ifelse(logit.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

confusion matrix on the test data using optimal cutoff based on kappa (0.39)
```{r}
#get predicted probabilities for the test data
logit.test1.pred <- predict(logit1.final, newdata = test1, type = "response")
logit.test1.y <- ifelse(logit.test1.pred > 0.39, 1, 0) # using cutoff = 0.39
confusionMatrix(as.factor(logit.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

confusion matrix on the test data using optimal cutoff based on sensitivity/specificty balance (0.34)
```{r}
#get predicted probabilities for the test data
logit.test1.pred <- predict(logit1.final, newdata = test1, type = "response")
logit.test1.y <- ifelse(logit.test1.pred > 0.34, 1, 0) # using cutoff = 0.34
confusionMatrix(as.factor(logit.test1.y), as.factor(test1$newgen), 
                positive = "1")
```


### Variable Selection: Stepwise

stepwise 
```{r}
set.seed(123)
logit1.none <- glm(newgen ~ 1, data = train1, family = binomial)
step1 <- step(logit1.none, 
                 list(lower=formula(logit1.none), upper = formula(logit1)),
                 direction = "both",
                 trace=0)
formula(step1)
summary(step1)
```

confusion matrix on the training data:
```
step.train1.pred <- step1$fitted.values
step.train1.y <- ifelse(step.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.train1.y), as.factor(train1$newgen), 
                positive = "1")
```
find optimal cut off value
```{r, warning = FALSE}
step.out1 <- opt.cut.genn(step1, valid1)
opt.cut.plot(step.out1)
#optimal by kappa score
step.out1$cutoff[which.max(step.out1$kappa.vec)]
#optimal by sensitivity/specificity balance
step.out1$cutoff[which.min(step.out1$ssdiff.vec)]
```

final stepwise model
```{r}
set.seed(123)
logit1.none <- glm(newgen ~ 1, data = all.train1, family = binomial)
step1.final <- step(logit1.none, 
                 list(lower=formula(logit1.none), upper = formula(logit1)),
                 direction = "both",
                 trace=0)
formula(step1.final)
summary(step1.final)
```


Stepwise Logistic Model:

Compared to the full model, the stepwise logistic model does not have Tempo or Energy in the model

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.226 + 0.079{Popularity} - 0.009Duration + 0.739{Acousticness} + 0.632{Danceability} 
\\ + 2.516{Instrumentalness} - 0.415{Key1} + 0.002{Key2} 
\\ + 0.147{Key3} + 0.279{Key4} - 0.222{Key5} - 0.276{Key6} - 0.162{Key7} 
\\ - 0.187{Key8} + 0.002{Key9} - 0.072{Key10} - 0.094{Key11} + 0.178{Loudness}
\\ + 0.980{Speechiness} - 0.668{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness, and choosing Key2(D major), Key3(D#/E-flat Major), Key4(E Major), Key9(A Major), instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key1(C#/D-flat Major), Key5(F Major), Key6(F#/G-flat Major), Key8 (G#/A-flat Major), Key10(A#/B-flat Major), Key11(B Major) instead of Key0(C Major).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Loudness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation for songs composed in the major mode. Unlike the full model, none of the key dummy variables are considered significant towards the predictive power of the model.


predict and evaluate on test using standard cut off 0.5
```{r}
#get predicted probabilities for the test data
step.test1.pred <- predict(step1.final, newdata = test1, type = "response")
step.test1.y <- ifelse(step.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on test using optimal cut off 0.39 for highest kappa
```{r}
#get predicted probabilities for the test data
step.test1.pred <- predict(step1.final, newdata = test1, type = "response")
step.test1.y <- ifelse(step.test1.pred > 0.39, 1, 0) # using cutoff = 0.39
confusionMatrix(as.factor(step.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on test using optimal cut off 0.34 for best sensitivity/specificity balance
```{r}
#get predicted probabilities for the test data
step.test1.pred <- predict(step1.final, newdata = test1, type = "response")
step.test1.y <- ifelse(step.test1.pred > 0.34, 1, 0) # using cutoff = 0.34
confusionMatrix(as.factor(step.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
bye3 <- capture.output(step1_kcv <- train(newgen ~ ., data = train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE)) 
```

```{r}
step1_kcv <- step1_kcv$finalModel
summary(step1_kcv)
```


```
#get predicted probabilities for training data
step_kcv.pred1 <- predict(step1_kcv, newdata = key.dummy(test1), type = "response")
step_kcv.train1.y <- ifelse(step_kcv.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.train1.y), as.factor(train1$newgen), 
                positive = "1")
```

find optimal cut off value
```{r, warning = FALSE}
step_kcv.out1 <- opt.cut.genn(step1_kcv, key.dummy(valid1))
opt.cut.plot(step_kcv.out1)
#optimal by kappa score
step_kcv.out1$cutoff[which.max(step_kcv.out1$kappa.vec)]
#optimal by sensitivity/specificity balance
step_kcv.out1$cutoff[which.min(step_kcv.out1$ssdiff.vec)]
```

Fit final model on combination of training and validation data
```{r}
set.seed(123)
# Fit K-fold CV model  
bye4 <- capture.output(step1_kcv.final <- train(newgen ~ ., data = all.train1, family = "binomial", 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE)) 
```

```{r}
step1_kcv.final <- step1_kcv.final$finalModel
summary(step1_kcv.final)
```

Stepwise 10CV Logistic Model:

Energy, Key2, Key3, Key9, Key10, Key11, Energy and Tempo not included.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.233 + 0.079{Popularity} - 0.009{Duration} + 0.743{Acousticness} + 0.621{Danceability} 
\\ + 2.524{Instrumentalness} - 0.406{Key1} + 0.287{Key4} - 0.214{Key5} 
\\ - 0.268{Key6} - 0.154{Key7} - 0.179{Key8} + 0.178{Loudness}
\\ + 0.992{Speechiness} - 0.674{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness, and choosing Key4(E Major) instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key1(C#/D-flat Major), Key5(F Major), Key6(F#/G-flat Major), Key8(G#/A-flat Major), instead of Key0(C Major).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Loudness, Valence, and choosing Key1(C#/D-flat Major) or Key6(F#/G-flat Major) instead of Key0 (C Major) are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation for songs composed in the major mode.


Get prediction and performance evaluation for standard cut off 0.5
```{r}
#get predicted probabilities for the test data
step_kcv.test1.pred <- predict(step1_kcv.final, newdata = key.dummy(test1), type = "response")
step_kcv.test1.y <- ifelse(step_kcv.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

Get prediction and performance evaluation for optimal cut off 0.39 for highest kappa
```{r}
#get predicted probabilities for the test data
step_kcv.test1.pred <- predict(step1_kcv.final, newdata = key.dummy(test1), type = "response")
step_kcv.test1.y <- ifelse(step_kcv.test1.pred > 0.39, 1, 0) # using cutoff = 0.39
confusionMatrix(as.factor(step_kcv.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

Get prediction and performance evaluation for optimal cut off 0.34 for best sensitivity/specificity balance
```{r}
#get predicted probabilities for the test data
step_kcv.test1.pred <- predict(step1_kcv.final, newdata = key.dummy(test1), type = "response")
step_kcv.test1.y <- ifelse(step_kcv.test1.pred > 0.34, 1, 0) # using cutoff = 0.34
confusionMatrix(as.factor(step_kcv.test1.y), as.factor(test1$newgen), 
                positive = "1")
```


### Variable Selection: All possible regression

```{r}
set.seed(123)
glmulti.out1 <- glmulti(newgen ~ ., data = train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out1@formulas
```

view summary of top model
```{r}
summary(glmulti.out1@objects[[1]])
```

Store model
```{r}
allreg.logit1 <- glmulti.out1@objects[[1]]
```


try fitting training data to results from all possible regression variable selection
```
#get predicted probabilities for training data
all.train1.pred <- allreg.logit1$fitted.values
all.train1.y <- ifelse(all.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.train1.y), as.factor(train1$newgen), 
                positive = "1")
```

find optimal cut off value
```{r, warning = FALSE}
allreg.out1 <- opt.cut.genn(allreg.logit1, valid1)
opt.cut.plot(allreg.out1)
#optimal by kappa score
allreg.out1$cutoff[which.max(allreg.out1$kappa.vec)]
#optimal by sensitivity/specificity balance
allreg.out1$cutoff[which.min(allreg.out1$ssdiff.vec)]
```

Fit final model on combo of train/validation data
```{r}
set.seed(123)
glmulti.out1.final <- glmulti(newgen ~ ., data = all.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out1.final@formulas
summary(glmulti.out1.final@objects[[1]])
allreg.logit1.final <- glmulti.out1.final@objects[[1]]
```

All Possible Regressions Model

Compared to the Full model, the one resulting from All possible Regressions omits Energy and Tempo.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.226 + 0.079{Popularity} - 0.009Duration + 0.739{Acousticness} + 0.632{Danceability} 
\\ + 2.516{Instrumentalness} - 0.415{Key1} + 0.002{Key2} 
\\ + 0.147{Key3} + 0.279{Key4} - 0.222{Key5} - 0.276{Key6} - 0.162{Key7} 
\\ - 0.187{Key8} + 0.002{Key9} - 0.072{Key10} - 0.094{Key11} + 0.178{Loudness}
\\ + 0.980{Speechiness} - 0.668{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Instrumentalness, Loudness, Speechiness, and choosing Key2(D major), Key3(D#/E-flat Major), Key4(E Major), Key9(A Major), instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key1(C#/D-flat Major), Key5(F Major), Key6(F#/G-flat Major), Key8 (G#/A-flat Major), Key10(A#/B-flat Major), Key11(B Major) instead of Key0(C Major).

Only the variables Popularity, Duration, Acousticness, Instrumentalness, Key1(C#/D-flat major), Loudness, and Valence are significant predictors in contributing to the log of odds for categorizing a song into the newer generations of kpop versus the older generation for songs composed in the major mode. 



```{r}
#get predicted probabilities for the test data
all.test1.pred <- predict(allreg.logit1.final, newdata = test1, type = "response")
all.test1.y <- ifelse(all.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

Using the cut off that yields the optimal kappa (0.39)
```{r}
#get predicted probabilities for the test data
all.test1.pred <- predict(allreg.logit1.final, newdata = test1, type = "response")
all.test1.y <- ifelse(all.test1.pred > 0.39, 1, 0) # using cutoff = 0.39
confusionMatrix(as.factor(all.test1.y), as.factor(test1$newgen), 
                positive = "1")
```

Using the cut-off that yields the best balance between the sensitivity and specificity rates. (0.34)
```{r}
#get predicted probabilities for the test data
all.test1.pred <- predict(allreg.logit1.final, newdata = test1, type = "response")
all.test1.y <- ifelse(all.test1.pred > 0.34, 1, 0) # using cutoff = 0.34
confusionMatrix(as.factor(all.test1.y), as.factor(test1$newgen), 
                positive = "1")
```


### Regularized Regression: Ridge 10 fold Cross Validation
```{r}
set.seed(123)
ridge1 <- train(newgen ~ ., data = train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1$bestTune
ridge1.model <- coef(ridge1$finalModel, ridge1$bestTune$lambda)
ridge1.model
```

Search for best cutoff using validation set
```{r, warning = FALSE}
ridge1.out <- reg.opt.cut.genn(ridge1, valid1)
opt.cut.plot(ridge1.out)
# cut off by kappa
ridge1.out$cutoff[which.max(ridge1.out$kappa.vec)]
ridge1.out$cutoff[which.min(ridge1.out$ssdiff.vec)]
```

create final model
```{r}
set.seed(123)
ridge1.final <- train(newgen ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 0,lambda = seq(0.001,0.1,by = 0.001)))
#best tuning parameter
ridge1.final$bestTune
ridge1.model.final <- coef(ridge1.final$finalModel, ridge1.final$bestTune$lambda)
ridge1.model.final
```


Ridge 10 Fold Cross Validation Logistic model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.538 + 0.062{Popularity} - 0.007{Duration} + 0.553{Acousticness} + 0.350{Danceability} 
\\ + 0.318{Energy} + 1.609{Instrumentalness} - 0.257{Key1} + 0.064{Key2} 
\\ + 0.205{Key3} + 0.256{Key4} - 0.121{Key5} - 0.162{Key6} - 0.056{Key7} 
\\ - 0.071{Key8} + 0.068{Key9} - 0.025{Key10} - 0.011{Key11} + 0.117{Loudness}
\\ + 0.915{Speechiness} + 0.0005{Tempo} - 0.502{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Danceability, Energy, Instrumentalness, Loudness, Speechiness, Tempo and choosing Key2(D major), Key3(D#/E-flat Major), Key4(E Major), Key9(A Major), instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key1(C#/D-flat Major), Key5(F Major), Key6(F#/G-flat Major), Key8 (G#/A-flat Major), Key10(A#/B-flat Major), Key11(B Major) instead of Key0(C Major).


predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.ridge1 <- predict(ridge1.final, s = ridge1.final$bestTune, test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.ridge1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.40 corresponding to optimal kappa
```{r}
prob.ridge1 <- predict(ridge1.final, s = ridge1.final$bestTune, test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.40, 1, 0) # using cutoff = 0.40
confusionMatrix(as.factor(pred.ridge1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.35 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.ridge1 <- predict(ridge1.final, s = ridge1.final$bestTune, test1, type = "prob")
pred.ridge1 <- ifelse(prob.ridge1[,2] > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(pred.ridge1), as.factor(test1$newgen), 
                positive = "1")
```


### Regularized Regression: Lasso 10 fold Cross Validation
```{r}
set.seed(123)
lasso1 <- train(newgen ~ ., data = train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso1$bestTune
# best coefficient
lasso1.model <- coef(lasso1$finalModel, lasso1$bestTune$lambda)
lasso1.model
```

Search for best cutoff using validation set
```{r, warning=FALSE}
lasso1.out <- reg.opt.cut.genn(lasso1, valid1)
opt.cut.plot(lasso1.out)
# cut off by kappa
lasso1.out$cutoff[which.max(lasso1.out$kappa.vec)]
lasso1.out$cutoff[which.min(lasso1.out$ssdiff.vec)]
```

create final model
```{r}
set.seed(123)
lasso1.final <- train(newgen ~ ., data = train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = 1,lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
lasso1.final$bestTune
# best coefficient
lasso1.model.final <- coef(lasso1.final$finalModel, lasso1.final$bestTune$lambda)
lasso1.model.final
```


Lasso 10 Fold Cross Validation Logistic model:

Compared to the full logistic model the lasso model does not include the variables Danceability, Energy, Key2, Key3, Key5 - Key11, Speechiness, and Tempo.

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.928 + 0.075{Popularity} - 0.006{Duration} + 0.195{Acousticness}
\\ + 1.169{Instrumentalness} - 0.173{Key1} + 0.002{Key4} + 0.081{Loudness} - 0.098{Valence}
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Acousticness, Instrumentalness, Loudness, and choosing Key4(E Major) instead of Key0 (C Major). The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration, Valence and choosing, Key1(C#/D-flat) instead of Key0(C Major).


predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.lasso1 <- predict(lasso1.final, s = lasso1.final$bestTune, test1, type = "prob")
pred.lasso1 <- ifelse(prob.lasso1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.lasso1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.44 corresponding to optimal kappa
```{r}
prob.lasso1 <- predict(lasso1.final, s = lasso1.final$bestTune, test1, type = "prob")
pred.lasso1 <- ifelse(prob.lasso1[,2] > 0.44, 1, 0) # using cutoff = 0.44
confusionMatrix(as.factor(pred.lasso1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.34 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.lasso1 <- predict(lasso1.final, s = lasso1.final$bestTune, test1, type = "prob")
pred.lasso1 <- ifelse(prob.lasso1[,2] > 0.34, 1, 0) # using cutoff = 0.34
confusionMatrix(as.factor(pred.lasso1), as.factor(test1$newgen), 
                positive = "1")
```


### Regularized Regression: Elastic Net 10 fold Cross Validation
```{r}
set.seed(123)
enet1 <- train(newgen ~ ., data = train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1$bestTune
# best coefficient
enet1.model <- coef(enet1$finalModel, enet1$bestTune$lambda)
enet1.model
```

search for best cutoff with validation set
```{r, warning = FALSE}
enet1.out <- reg.opt.cut.genn(enet1, valid1)
opt.cut.plot(enet1.out)
# cut off by kappa
enet1.out$cutoff[which.max(enet1.out$kappa.vec)]
enet1.out$cutoff[which.min(enet1.out$ssdiff.vec)]
```

create final model on train and validation
```{r}
set.seed(123)
enet1.final <- train(newgen ~ ., data = all.train1, method = "glmnet",
                      family = "binomial", trControl = train_control, 
                      tuneGrid = expand.grid(alpha = seq(0,1,by = 0.05),lambda = seq(0.001,0.1,by = 0.001)))
# best parameter
enet1.final$bestTune
# best coefficient
enet1.model.final <- coef(enet1.final$finalModel, enet1.final$bestTune$lambda)
enet1.model.final
```


Elastic Net 10 Fold Cross Validation Logistic model:

Compared to the full logistic model the lasso model does not include the variables Acousticness, Danceability, Energy, Key2 - Key11, Speechiness, Duration and Tempo.
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.072 + 0.071{Popularity} - 0.005{Duration} 
\\ + 0.671{Instrumentalness} - 0.059{Key1} + 0.054{Loudness} 
$$

The audio features in this model that increase the odds of being categorized as a new generation kpop song multiplicativley are Popularity, Instrumentalness, and Loudness. The features that decrease the odds multiplicatively of being categorized as a new generation kpop song are Duration and choosing Key1(C#/D-flat) instead of Key0(C Major).


predict and evaluate on the test set where cutoff is at 0.5
```{r}
prob.enet1 <- predict(enet1.final, s = enet1.final$bestTune, test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.50, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.enet1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.43 corresponding to optimal kappa
```{r}
prob.enet1 <- predict(enet1.final, s = enet1.final$bestTune, test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.43, 1, 0) # using cutoff = 0.43
confusionMatrix(as.factor(pred.enet1), as.factor(test1$newgen), 
                positive = "1")
```

predict and evaluate on the test set where cutoff is at 0.35 corresponding to optimal balance of sensitivity and specificity
```{r}
prob.enet1 <- predict(enet1.final, s = enet1.final$bestTune, test1, type = "prob")
pred.enet1 <- ifelse(prob.enet1[,2] > 0.35, 1, 0) # using cutoff = 0.35
confusionMatrix(as.factor(pred.enet1), as.factor(test1$newgen), 
                positive = "1")
```

### Deciding on the best model for Mode 1 songs



Performance metrics
```{r}
mode1.results <- data.frame(Model = c("Full", "Step", "Step 10CV", "AllReg", "Ridge 10CV", "Lasso 10CV", "ElasticNet 10CV"),
                            cutoff = c(0.34, 0.34, 0.34, 0.34, 0.35, 0.34, 0.35), #Best balance between sensitivity and specificity
                            AIC = c(5936.1, 5934, 5925.4, 5934, NA, NA, NA),
                            Accuracy = c(0.7575, 0.7566, 0.7557, 0.7566, 0.7566, 0.7520, 0.7566),
                            Kappa = c(0.5006, 0.4995, 0.4974, 0.4995, 0.4985, 0.4894, 0.4952),
                            Sensitivity = c(0.7689, 0.7713, 0.7689, 0.7713, 0.7664, 0.7616, 0.7494),
                            Specificity = c(0.7507, 0.7478, 0.7478, 0.7478, 0.7507, 0.7464, 0.7609))
mode1.results
```

The best model is the full logistic model.

Full Logistic model 

$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.783 + 0.079{Popularity} - 0.009{Duration} + 0.882{Acousticness} + 0.665{Danceability} 
\\ + 0.537{Energy} + 2.471{Instrumentalness} - 0.415{Key1} + 0.002{Key2} 
\\ + 0.149{Key3} + 0.280{Key4} - 0.217{Key5} - 0.280{Key6} - 0.161{Key7} 
\\ - 0.185{Key8} + 0.006{Key9} - 0.071{Key10} - 0.094{Key11} + 0.154{Loudness}
\\ + 0.792{Speechiness} + 0.0002{Tempo} - 0.742{Valence}
$$