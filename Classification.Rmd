---
title: "Classification (Supervised Learning)"
author: "Mary Solomon"
date: "2/9/2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales) #for formatting with percentage signs
library(nnet) #for multinomial regression
library(caret)
library(leaps) # for all possible regressions approach LINEAR
library(glmulti)# for all possible regressions approach LOGISTIC
```




set levels of ordinal data, and other variables to their appropriate dataypes. Did not adjust the datatypes for date/time since those will not be used in this portion
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               #release_date = as.POSIXct.Date(release_date),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)))
data$gen1 <- ifelse(data$ArtistGen == 1, 1, 0)
data$oldgen <- ifelse(data$ArtistGen == 1 | data$ArtistGen == 2, 1, 0)
```


Variables being kept for classification and cluster analysis: popularity, duration, acousticness, danceability, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, valence. These are the predictor variables that will be used to classify on outcome of ArtistGeneration, or to determine the optimal clusters in the case of a clustering problem. 
Any information about date of song release or date of artist promotions are excluded since these are directly correlated to kpop generations. rather, we just want to make predictions on whether we can classify the kpop songs into their kpop generations based on qualities of the music such as the audio features.

Time signature is out since almost all are 4/4 time. Analysis will also be split up into logistic regression predicting for songs where mode = 0 and those for where mode = 1

Personal predictions: I believe popularity will be a strong predictor since the popularity measure is also time dependent.




# Logistic: predict which are gen1 or not  gen 1 songs
Create Training (75%) and Test data (25%) to train classification models on.
```{r}
kpop <- select(data, gen1, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
g1.kpop0 <- kpop %>% filter(mode == 0)%>% select(-mode)
g1.kpop1 <- kpop %>% filter(mode == 1) %>% select(-mode)

### Kpop mode 0 train and test
g1.smpl.size0 <- floor(0.75*nrow(g1.kpop0))
set.seed(123)
g1.smpl0 <- sample(nrow(g1.kpop0), g1.smpl.size0, replace = FALSE)
g1.train0 <- g1.kpop0[g1.smpl0,]
g1.test0 <- g1.kpop0[-g1.smpl0,]

### Kpop mode 1 train and test
g1.smpl.size1 <- floor(0.75*nrow(g1.kpop1))
set.seed(123)
g1.smpl1 <- sample(nrow(g1.kpop1), g1.smpl.size1, replace = FALSE)
g1.train1 <- g1.kpop1[g1.smpl1,]
g1.test1 <- g1.kpop1[-g1.smpl1,]
```


## Logistic Where mode = 0
Minor key
```{r}
g1.logit0 <- glm(gen1 ~., data = g1.train0, family = "binomial")
summary(g1.logit0)
```

confusion matrix on the training data:
```{r}
g1.tain0.pred <- g1.logit0$fitted.values
g1.train0.y <- ifelse(g1.tain0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train0.y), as.factor(g1.train0$gen1), 
                positive = "1")
```


### Variable Selection on the logistic model for mode = 0 songs to predict 1st generation or not.

stepwise
```{r}
g1.logit0.none <- glm(gen1 ~ 1, data = g1.train0, family = binomial)
g1.step0 <- step(g1.logit0.none, 
                 list(lower=formula(g1.logit0.none), upper = formula(g1.logit0)),
                 direction = "both",
                 trace=0)
formula(g1.step0)
summary(g1.step0)
```

confusion matrix on the training data:
```{r}
g1.tain0.pred <- g1.step0$fitted.values
g1.train0.y <- ifelse(g1.tain0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train0.y), as.factor(g1.train0$gen1), 
                positive = "1")
```



All possible regressions selection using the glmulti() package based on AIC.
```{r}
glmulti.g1.out0 <- glmulti(gen1 ~ ., data = g1.train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.g1.out0@formulas
```

```{r}
summary(glmulti.g1.out0@objects[[1]])
```

```{r}
#summary(glmulti.g1.out0@objects[[2]])
allreg.g1.logit0 <- glmulti.g1.out0@objects[[1]]
```


try fitting training data to results from all possible regression variable selection
```{r}
#get predicted probabilities for training data
g1.tain0.pred <- allreg.g1.logit0$fitted.values
g1.train0.y <- ifelse(g1.tain0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train0.y), as.factor(g1.train0$gen1), 
                positive = "1")
```


### Deciding on the best model
```{r}
g1.models0 <- data.frame(Model = c("Full", "Stepwise", "AllReg"),
                         Accuracy = c(0.8602, 0.8593, 0.8593),
                         CI = c("(0.8482, 0.8715)", "(0.8473, 0.8707)", "(0.8473, 0.8707)"),
                         Sensitivity = c(0.35593, 0.35424, 0.35424),
                         Specificity = c(0.96225, 0.96156, 0.96156),
                         Kappa = c(0.3892, 0.386,0.386))
g1.models0
```

We can see that the Stepwise and All possible Regressions variable selection methods yielded the same optimal model this model, in turn was quite similar to the Full model. Ultimately, the Full model has the best performanceo n the training dataset as it has the highest measures for accuracy, sensitivity, specificity and kappa value. 


### Run final model on the test data and predict
confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
g1.test0.pred <- predict(g1.logit0, newdata = g1.test0, type = "response")
g1.test0.y <- ifelse(g1.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.test0.y), as.factor(g1.test0$gen1), 
                positive = "1")
```





## Logistic Where mode = 1

Major key
```{r}
g1.logit1 <- glm(gen1 ~., data = g1.train1, family = "binomial")
summary(g1.logit1)
```



confusion matrix on the training data:
```{r}
g1.tain1.pred <- g1.logit1$fitted.values
g1.train1.y <- ifelse(g1.tain1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train1.y), as.factor(g1.train1$gen1), 
                positive = "1")
```


### Variable Selection on the logistic model for mode = 1 songs to predict 1st generation or not.

stepwise
```{r}
g1.logit1.none <- glm(gen1 ~ 1, data = g1.train1, family = binomial)
g1.step1 <- step(g1.logit1.none, 
                 list(lower=formula(g1.logit1.none), upper = formula(g1.logit1)),
                 direction = "both",
                 trace=0)
formula(g1.step1)
summary(g1.step1)
```


confusion matrix on the training data:
```{r}
g1.tain1.pred <- g1.step1$fitted.values
g1.train1.y <- ifelse(g1.tain1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train1.y), as.factor(g1.train1$gen1), 
                positive = "1")
```


All possible regressions selection using the glmulti() package based on AIC.
```{r}
glmulti.g1.out1 <- glmulti(gen1 ~ ., data = g1.train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.g1.out1@formulas
```

```{r}
summary(glmulti.g1.out1@objects[[1]])
allreg.g1.logit1 <- glmulti.g1.out1@objects[[1]]
```

try fitting training data to results from all possible regression variable selection
```{r}
#get predicted probabilities for training data
g1.tain1.pred <- allreg.g1.logit1$fitted.values
g1.train1.y <- ifelse(g1.tain1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.train1.y), as.factor(g1.train1$gen1), 
                positive = "1")
```


### Deciding on the best Logistic model for songs where mode = 1
```{r}
g1.models1 <- data.frame(Model = c("Full", "Stepwise", "AllReg"),
                         Accuracy = c(0.8561, 0.8556, 0.8556),
                         CI = c("(0.8466, 0.8653)", "(0.846, 0.8647)", "(0.846, 0.8647)"),
                         Sensibility = c(0.24830, 0.24603, 0.24603),
                         Specificity = c(0.97209, 0.97187, 0.97187),
                         Kappa = c(0.2919, 0.2888, 0.2888))
g1.models1
```

Similar to the varaible selection methods for choosing the best logistic model for songs where mode = 0. The optimal model chosen from the stepwise and all possible regression variable selection methods are identical and very similar to the full model. However, the full model is still the best as it has the highest rates for diagnostic measures. 

### Run final model on the test data and predict
confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
g1.test1.pred <- predict(g1.logit1, newdata = g1.test1, type = "response")
g1.test1.y <- ifelse(g1.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(g1.test1.y), as.factor(g1.test1$gen1), 
                positive = "1")
```


As compared to K-pop songs where the mode is in a minor key, the songs that have a major key has a slightly lower accuracy and other assessment measure rates. Therefore, for the songs in a major key, the optimal logistic model is not as successful as classifying a song into being from the first generation or the 2-4th generations. 

Overall, for both models, it is important to note that the sensitivity rate in being able to successfully classify songs from generation 1 is very low: 0.23311 for songs in a major key and 0.35593 for songs in a minor key. Some explanation for this could be that a handful of generation 1 kpop artists have still released successful music from 2009 to today. Therefore, some of the kpop songs from an artist that would be classified as being a generation 1 idol, are likely going to share similarities with songs being released in later generations. Some key examples of this are: JYP, BoA, and SHINHWA who are still releasing successful tracks in the modern generations.

This result is motivation to look at the relationship between song attributes and release dates rather than their classified kpop generation.




## Logistic: predict which are old gen vs new gen
Create Training (75%) and Test data (25%) to train classification models on.
```{r}
kpop <- select(data, oldgen, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% filter(mode == 0)%>% select(-mode)
kpop1 <- kpop %>% filter(mode == 1) %>% select(-mode)

### Kpop mode 0 train and test
smpl.size0 <- floor(0.75*nrow(kpop0))
set.seed(123)
smpl0 <- sample(nrow(kpop0), smpl.size0, replace = FALSE)
train0 <- kpop0[smpl0,]
test0 <- kpop0[-smpl0,]

### Kpop mode 1 train and test
smpl.size1 <- floor(0.75*nrow(kpop1))
set.seed(123)
smpl1 <- sample(nrow(kpop1), smpl.size1, replace = FALSE)
train1 <- kpop1[smpl1,]
test1 <- kpop1[-smpl1,]
```


### Logistic Where mode = 0
Minor key
```{r}
logit0 <- glm(oldgen ~., data = train0, family = "binomial")
summary(logit0)
```


confusion matrix on the training data:
```{r}
tain0.pred <- logit0$fitted.values
train0.y <- ifelse(tain0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(train0.y), as.factor(train0$oldgen), 
                positive = "1")
```




confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
test0.pred <- predict(logit0, newdata = test0, type = "response")
test0.y <- ifelse(test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(test0.y), as.factor(test0$oldgen), 
                positive = "1")
```


### Logistic Where mode = 1

Major key
```{r}
logit1 <- glm(oldgen ~., data = train1, family = "binomial")
summary(logit1)
```


confusion matrix on the training data:
```{r}
tain1.pred <- logit1$fitted.values
train1.y <- ifelse(tain1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(train1.y), as.factor(train1$oldgen), 
                positive = "1")
```


confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
test1.pred <- predict(logit1, newdata = test1, type = "response")
test1.y <- ifelse(test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(test1.y), as.factor(test1$oldgen), 
                positive = "1")
```

### Multinomial Logistic Regression ###
#### Assumptions


#### Model building 
run on the training data.
```{r}
data<- select(data, ArtistGen, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
multi.train <- multinom(ArtistGen ~ ., data = data)
summary(multi.train)
```

Get p-values for the significance of each predictor variable in the model.
```{r}
z <- summary(multi.train)$coefficients/summary(multi.train)$standard.errors
#z
### Using Wald Z-test (need to check assumptions)###
# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```
Variables that are not significant are: key1, key2, key 4, key5, key6, key8, key9, key11, mode1, 


#### prediction