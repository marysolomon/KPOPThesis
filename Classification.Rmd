---
title: "Classification (Supervised Learning): Logistic Regression"
author: "Mary Solomon"
date: "2/9/2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(stringr)
library(scales) #for formatting with percentage signs
library(nnet) #for multinomial regression
library(caret)
library(leaps) # for all possible regressions approach LINEAR
library(glmulti)# for all possible regressions approach LOGISTIC
```


```{r}
key.dummy <- function(x){
  x <- x %>% mutate(key1 = ifelse(x$key == 1, 1, 0),
              key2 = ifelse(x$key == 2, 1, 0),
              key3 = ifelse(x$key == 3, 1, 0),
              key4 = ifelse(x$key == 4, 1, 0),
              key5 = ifelse(x$key == 5, 1, 0),
              key6 = ifelse(x$key == 6, 1, 0),
              key7 = ifelse(x$key == 7, 1, 0),
              key8 = ifelse(x$key == 8, 1, 0),
              key9 = ifelse(x$key == 9, 1, 0),
              key10 = ifelse(x$key == 10, 1, 0),
              key11 = ifelse(x$key == 11, 1, 0)) %>% select(-key)
    
  x
}
```


set levels of ordinal data, and other variables to their appropriate dataypes. Did not adjust the datatypes for date/time since those will not be used in this portion
```{r}
data <- fread("kpopdata.csv")
data <- mutate(data, ArtistType = as.factor(ArtistType),
               ArtistGender = as.factor(ArtistGender),
               ArtistGen = factor(ArtistGen),
               #release_date = as.POSIXct.Date(release_date),
               key = factor(key, levels = 0:11),
               mode = as.factor(mode),
               time_signature = factor(time_signature, levels = c(1,3,4,5)))
data$gen1 <- as.factor(ifelse(data$ArtistGen == 1, 1, 0))
data$oldgen <- as.factor(ifelse(data$ArtistGen == 1 | data$ArtistGen == 2, 1, 0))
```


Variables being kept for classification and cluster analysis: popularity, duration, acousticness, danceability, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, valence. These are the predictor variables that will be used to classify on outcome of ArtistGeneration, or to determine the optimal clusters in the case of a clustering problem. 
Any information about date of song release or date of artist promotions are excluded since these are directly correlated to kpop generations. rather, we just want to make predictions on whether we can classify the kpop songs into their kpop generations based on qualities of the music such as the audio features.

Time signature is out since almost all are 4/4 time. Analysis will also be split up into logistic regression predicting for songs where mode = 0 and those for where mode = 1

Personal predictions: I believe popularity will be a strong predictor since the popularity measure is also time dependent.


# Logistic: predict which songs are old gen (1 & 2) vs new gen (3 & 4)
Create Training (75%) and Test data (25%) to train classification models on.
```{r}
kpop <- select(data, oldgen, popularity, duration, acousticness, danceability, energy, instrumentalness, key, loudness, mode, speechiness, tempo, valence)
kpop0 <- kpop %>% filter(mode == 0)%>% select(-mode)
kpop1 <- kpop %>% filter(mode == 1) %>% select(-mode)

### Kpop mode 0 train and test
smpl.size0 <- floor(0.75*nrow(kpop0))
set.seed(123)
smpl0 <- sample(nrow(kpop0), smpl.size0, replace = FALSE)
train0 <- kpop0[smpl0,]
test0 <- kpop0[-smpl0,]

### Kpop mode 1 train and test
smpl.size1 <- floor(0.75*nrow(kpop1))
set.seed(123)
smpl1 <- sample(nrow(kpop1), smpl.size1, replace = FALSE)
train1 <- kpop1[smpl1,]
test1 <- kpop1[-smpl1,]
```


## Logistic Where mode = 0
Minor key

Full logistic model
```{r}
logit0 <- glm(oldgen ~., data = train0, family = "binomial")
summary(logit0)
```


confusion matrix on the training data:
```{r}
logit.train0.pred <- logit0$fitted.values
logit.train0.y <- ifelse(logit.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.train0.y), as.factor(train0$oldgen), 
                positive = "1")
```


confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
logit.test0.pred <- predict(logit0, newdata = test0, type = "response")
logit.test0.y <- ifelse(logit.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.test0.y), as.factor(test0$oldgen), 
                positive = "1")
```


### Variable Selection: Stepwise
stepwise 
```{r}
logit0.none <- glm(oldgen ~ 1, data = train0, family = binomial)
step0 <- step(logit0.none, 
                 list(lower=formula(logit0.none), upper = formula(logit0)),
                 direction = "both",
                 trace=0)
formula(step0)
summary(step0)
```

confusion matrix on the training data:
```{r}
step.train0.pred <- step0$fitted.values
step.train0.y <- ifelse(step.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.train0.y), as.factor(train0$oldgen), 
                positive = "1")
```


predict and evaluate on test
```{r}
#get predicted probabilities for the test data
step.test0.pred <- predict(step0, newdata = test0, type = "response")
step.test0.y <- ifelse(step.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.test0.y), as.factor(test0$oldgen), 
                positive = "1")
```

### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
step0_kcv <- train(oldgen ~ ., data = train0, family = "binomial", 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE) 
```

```{r}
step0_kcv <- step0_kcv$finalModel
step0_kcv
```


```{r}
#get predicted probabilities for training data
step_kcv.train0.pred <- step0_kcv$fitted.values
step_kcv.train0.y <- ifelse(step_kcv.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.train0.y), as.factor(train0$oldgen), 
                positive = "1")
```

```{r}
#get predicted probabilities for the test data
step_kcv.test0.pred <- predict(step0_kcv, newdata = key.dummy(test0), type = "response")
step_kcv.test0.y <- ifelse(step_kcv.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.test0.y), as.factor(test0$oldgen), 
                positive = "1")
```



### Variable Selection: All possible regression

```{r}
glmulti.out0 <- glmulti(oldgen ~ ., data = train0,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out0@formulas
```

view summary of top model
```{r}
summary(glmulti.out0@objects[[1]])
```

Store model
```{r}
allreg.logit0 <- glmulti.out0@objects[[1]]
```


try fitting training data to results from all possible regression variable selection
```{r}
#get predicted probabilities for training data
all.train0.pred <- allreg.logit0$fitted.values
all.train0.y <- ifelse(all.train0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.train0.y), as.factor(train0$oldgen), 
                positive = "1")
```

```{r}
#get predicted probabilities for the test data
all.test0.pred <- predict(allreg.logit0, newdata = test0, type = "response")
all.test0.y <- ifelse(all.test0.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.test0.y), as.factor(test0$oldgen), 
                positive = "1")
```

### Deciding on the best model for Mode 0 songs

Full Logistic model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -0.223 - 0.073x_{Popularity} + 0.521x_{Duration} - 1.398x_{Acousticness} - 0.594x_{Danceability} 
\\ - 0.587x_{Energy} - 2.510x_{Instrumentalness} - 0.107x_{Key1} - 0.136x_{Key2} 
\\ + 0.582x_{Key3} + 0.283x_{Key4} + 0.247x_{Key5} + 0.218x_{Key6} - 0.441x_{Key7} 
\\ + 0.213x_{Key8} + 0.333x_{Key9} + 0.391x_{Key10} + 0.184x_{Key11} - 0.204x_{Loudness}
\\ -1.954x_{Speechiness} + 0.003x_{Tempo} + 1.376x_{Valence}
$$

Stepwise Logistic Model:
In comparison to the full model, energy and danceability are not included
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.219 - 0.073x_{Popularity} + 0.534x_{Duration} - 1.187x_{Acousticness} 
\\ -2.515x_{Instrumentalness} - 0.092x_{Key1} - 0.125x_{Key2} 
\\ + 0.598x_{Key3} + 0.275x_{Key4} + 0.254x_{Key5} + 0.233x_{Key6} - 0.435x_{Key7} 
\\ + 0.225x_{Key8} + 0.338x_{Key9} + 0.395x_{Key10} + 0.186x_{Key11} - 0.225x_{Loudness}
\\ -2.006x_{Speechiness} + 0.004x_{Tempo} + 1.184x_{Valence}
$$

Stepwise 10kcv Logistic Model
compared to The full model, energy, danceability, Key1, key2, and key8 are not included in this model. Code would not run predictions on the test data so I don't have fair performance metrics for this model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.251 - 0.072x_{Popularity} + 0.538x_{Duration} - 1.172x_{Acousticness} 
\\ -2.520x_{Instrumentalness} 
\\ + 0.610x_{Key3} + 0.287x_{Key4} + 0.267x_{Key5} + 0.246x_{Key6} - 0.422x_{Key7} 
\\ 0.351x_{Key9} + 0.408x_{Key10} + 0.199x_{Key11} - 0.226x_{Loudness}
\\ -2.000x_{Speechiness} + 0.004x_{Tempo} + 1.196x_{Valence}
$$

All Possible Regressions Model
Danceability and energy are not included in this model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = -1.219 - 0.073x_{Popularity} + 0.534x_{Duration} - 1.187x_{Acousticness} 
\\ - 2.515x_{Instrumentalness} - 0.092x_{Key1} - 0.125x_{Key2} 
\\ + 0.598x_{Key3} + 0.275x_{Key4} + 0.254x_{Key5} + 0.233x_{Key6} - 0.435x_{Key7} 
\\ + 0.225x_{Key8} + 0.338x_{Key9} + 0.395x_{Key10} + 0.186x_{Key11} - 0.225x_{Loudness}
\\ -2.006x_{Speechiness} + 0.004x_{Tempo} + 1.184x_{Valence}
$$

Performance metrics
```{r}
mode0.results <- data.frame(Model = c("Full", "Step", "Step 10kCV", "AllReg"),
                            AIC = c(3408.5, 3407, 3403, 3407),
                            Accuracy = c(0.7682, 0.7682, 0.7665, 0.7682),
                            Kappa = c(0.4902, 0.4902, 0.4864, 0.4902),
                            Sensitivity = c(0.8531, 0.8531, 0.8517, 0.8531),
                            Specificity = c(0.6244, 0.6244, 0.6221, 0.6244))
mode0.results
```

All models have exactly the same results. unsure about the cross validation approach however, due to technical difficulties.


## Logistic Where mode = 1

Major key

Full logistic regression model
```{r}
logit1 <- glm(oldgen ~., data = train1, family = "binomial")
summary(logit1)
```


confusion matrix on the training data:
```{r}
logit.train1.pred <- logit1$fitted.values
logit.train1.y <- ifelse(logit.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.train1.y), as.factor(train1$oldgen), 
                positive = "1")
```


confusion matrix on the test data
```{r}
#get predicted probabilities for the test data
logit.test1.pred <- predict(logit1, newdata = test1, type = "response")
logit.test1.y <- ifelse(logit.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(logit.test1.y), as.factor(test1$oldgen), 
                positive = "1")
```


### Variable Selection: Stepwise

stepwise 
```{r}
logit1.none <- glm(oldgen ~ 1, data = train1, family = binomial)
step1 <- step(logit1.none, 
                 list(lower=formula(logit1.none), upper = formula(logit1)),
                 direction = "both",
                 trace=0)
formula(step1)
summary(step1)
```

confusion matrix on the training data:
```{r}
step.train1.pred <- step1$fitted.values
step.train1.y <- ifelse(step.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.train1.y), as.factor(train1$oldgen), 
                positive = "1")
```


predict and evaluate on test
```{r}
#get predicted probabilities for the test data
step.test1.pred <- predict(step1, newdata = test1, type = "response")
step.test1.y <- ifelse(step.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step.test1.y), as.factor(test1$oldgen), 
                positive = "1")
```


### Variable Selection: Stepwise (10 fold CV)

step-wise 10 fold cross validation
```{r echo = TRUE, message=FALSE, result='hide', warning = FALSE, error = FALSE}
set.seed(123)
train_control <- trainControl(method = "cv", 
                              number = 10) 

# Fit K-fold CV model  
step1_kcv <- train(oldgen ~ ., data = train1, family = binomial(), 
                 method = "glmStepAIC", trControl = train_control, verbose = FALSE) 
```

```{r}
step1_kcv <- step1_kcv$finalModel
step1_kcv
```


```{r}
#get predicted probabilities for training data
step_kcv.pred1 <- predict(step1_kcv, newdata = key.dummy(test1), type = "response")
step_kcv.train1.y <- ifelse(step_kcv.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(step_kcv.train1.y), as.factor(train1$oldgen), 
                positive = "1")
```

```{r}
#get predicted probabilities for the test data
test1.pred <- predict(step1_kcv, newdata = key.dummy(test1), type = "response")
test1.y <- ifelse(test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(test1.y), as.factor(test1$oldgen), 
                positive = "1")
```


### Variable Selection: All possible regression

```{r}
glmulti.out1 <- glmulti(oldgen ~ ., data = train1,
                           level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # lm function
            family = binomial)       # binomial to run logistic regression 
glmulti.out1@formulas
```

view summary of top model
```{r}
summary(glmulti.out1@objects[[1]])
```

Store model
```{r}
allreg.logit1 <- glmulti.out1@objects[[1]]
```


try fitting training data to results from all possible regression variable selection
```{r}
#get predicted probabilities for training data
all.train1.pred <- allreg.logit1$fitted.values
all.train1.y <- ifelse(all.train1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.train1.y), as.factor(train1$oldgen), 
                positive = "1")
```

```{r}
#get predicted probabilities for the test data
all.test1.pred <- predict(allreg.logit1, newdata = test1, type = "response")
all.test1.y <- ifelse(all.test1.pred > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(all.test1.y), as.factor(test1$oldgen), 
                positive = "1")
```




### Deciding on the best model for Mode 1 songs

Full Logistic model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.852 - 0.080{Popularity} + 0.481{Duration} - 0.893{Acousticness} - 0.645{Danceability} 
\\ - 0.453{Energy} - 2.408{Instrumentalness} + 0.461{Key1} + 0.065{Key2} 
\\ - 0.095{Key3} -0.164{Key4} + 0.246{Key5} + 0.317{Key6} + 0.192{Key7} 
\\ + 0.280{Key8} -0.040{Key9} + 0.133{Key10} + 0.259{Key11} - 0.146{Loudness}
\\ -0.760{Speechiness} - 0.0003{Tempo} + 0.740{Valence}
$$

Stepwise Logistic Model
Compared to the full model, the stepwise logistic model does not have tempo or energy in the model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.346 - 0.080{Popularity} + 0.484Duration - 0.770{Acousticness} - 0.600{Danceability} 
\\ - 2.439{Instrumentalness} + 0.462{Key1} + 0.065{Key2} 
\\ - 0.093{Key3} -0.161{Key4} + 0.250{Key5} + 0.313{Key6} + 0.194{Key7} 
\\ + 0.282{Key8} -0.035{Key9} + 0.131{Key10} + 0.259{Key11} - 0.146{Loudness}
\\ -0.928{Speechiness} + 0.672{Valence}
$$

Stepwise 10kcv Logistic Model
Energy, key2, key3, key4, key 9, key 10, and tempo not included.
Code has issues running this model through prediction due to the dummy variables :(
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.322 - 0.080{Popularity} + 0.486{Duration} - 0.782{Acousticness} - 0.579{Danceability} 
\\ - 2.420{Instrumentalness} + 0.462{Key1}
\\ + 0.252{Key5} + 0.314{Key6} + 0.194{Key7} 
\\ + 0.283{Key8} + 0.261{Key11} - 0.167{Loudness}
\\ -0.909{Speechiness} + 0.673{Valence}
$$

All Possible Regressions Model
$$
log(\frac{\hat \pi}{1 - \hat \pi}) = 0.346 - 0.080{Popularity} + 0.484Duration - 0.770{Acousticness} - 0.600{Danceability} 
\\ - 2.439{Instrumentalness} + 0.462{Key1} + 0.065{Key2} 
\\ - 0.093{Key3} -0.161{Key4} + 0.250{Key5} + 0.313{Key6} + 0.194{Key7} 
\\ + 0.282{Key8} -0.036{Key9} + 0.131{Key10} + 0.259{Key11} - 0.167{Loudness}
\\ -0.928{Speechiness} + 0.672{Valence}
$$

Performance metrics
```{r}
mode1.results <- data.frame(Model = c("Full", "Step", "Step 10kCV", "AllReg"),
                            AIC = c(5243.3, 5240.5, 5233, 5240.5),
                            Accuracy = c(0.776, 0.776, 0.776, 0.7682),
                            Kappa = c(0.4952, 0.4956, 0.4956, 0.4902),
                            Sensitivity = c(0.8685, 0.8677, 0.8677, 0.8531),
                            Specificity = c(0.6098, 0.6113, 0.6113, 0.6244))
mode1.results
```

